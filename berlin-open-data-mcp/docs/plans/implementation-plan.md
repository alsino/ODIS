# Berlin Open Data MCP Server - Implementation Plan

## Overview

This plan details the implementation of an enhanced Berlin Open Data MCP server that goes beyond basic search to enable data fetching, sampling, analysis, and agentic workflow chaining. The implementation follows a phased approach with frequent commits and test-driven development principles.

## Key Implementation Decisions

Based on project requirements and architectural discussions:

1. **Keyword mapping strategy**: Keep focused mappings (current approach already finds 701 results for "verkehr"). Only expand if gaps emerge in actual usage.

2. **Category/organization filtering tools**: SKIP implementation. The `search_berlin_datasets` tool already handles these use cases more effectively than exact tag filtering.

3. **CSV parsing**: Use `papaparse` library from the start for robust handling of edge cases (delimiters, encoding, quotes, etc.).

4. **Git repository**: Initialize before starting implementation (Phase 0).

5. **Testing priority**: Focus on integration tests first. Manual testing with Claude Desktop is secondary and can be deferred.

6. **Total expected commits**: 10-12 across all phases (Phase 0: 2, Phase 1: 3, Phase 2: 6, Phase 3: 3-4)

## Query Expansion System Architecture

The MCP server uses a hybrid query expansion system to work around Berlin's CKAN API limitations (no wildcards, stemming, or fuzzy matching).

### How It Works

**Two-component system:**

1. **Manual seed mappings** (`src/query-processor.ts`):
   - Small curated list mapping user search terms ‚Üí portal-native terms
   - Example: `"miete": ["mietspiegel"]`, `"wohnung": ["wohnen", "wohn"]`
   - Handles terms that don't appear in portal metadata

2. **Generated expansions** (`src/generated-expansions.ts`):
   - Automatically generated from analyzing 2,660 portal datasets
   - High-quality mappings created by `scripts/generate-query-expansion.ts`
   - Uses frequency-based ranking, co-occurrence filtering, negation filtering, and redundancy elimination
   - Example: `"mietspiegel": ["Mietspiegel", "Mietspiegels", "Mietspiegeldatenbank"]`

**Runtime merging** (happens in `QueryProcessor` constructor):
```typescript
constructor() {
  this.QUERY_EXPANSION = { ...QUERY_EXPANSION }; // Start with generated map

  // Expand each seed mapping recursively
  for (const [userTerm, portalTerms] of Object.entries(this.SEED_MAPPINGS)) {
    const expandedTerms = new Set<string>();

    for (const portalTerm of portalTerms) {
      if (QUERY_EXPANSION[portalTerm]) {
        // Portal term has expansions ‚Üí use them
        QUERY_EXPANSION[portalTerm].forEach(t => expandedTerms.add(t));
      } else {
        // No expansion ‚Üí capitalize and use directly
        expandedTerms.add(capitalize(portalTerm));
      }
    }

    this.QUERY_EXPANSION[userTerm] = Array.from(expandedTerms);
  }
}
```

**Example: User searches "wohnung"**

1. Seed mapping: `"wohnung" ‚Üí ["wohnen", "wohn"]`
2. Look up "wohnen" in generated map ‚Üí NOT FOUND (eliminated as redundant with "wohn")
   - Fallback: capitalize ‚Üí `"Wohnen"`
3. Look up "wohn" in generated map ‚Üí FOUND: `["Wohngeb√§ude", "Wohn- und nichtwohngeb√§ude", ...]`
4. Final expansion: `["Wohnen", "Wohngeb√§ude", "Wohn- und nichtwohngeb√§ude", ...]`

**Why "wohnen" isn't in the generated map:**
The generation algorithm processes words by length (shorter first) and skips longer forms as redundant. Since "wohn" (4 chars) was processed before "wohnen" (6 chars), "wohnen" was skipped as redundant. This consolidates the 20+ "wohn*" word family into a single high-quality entry.

**Why frequency-based ranking (not PMI):**
The algorithm ranks compound words by dataset frequency, not PMI (Pointwise Mutual Information). This is because:

1. **Compound words always co-occur**: "Wohngeb√§ude" always appears with "wohn" (it contains it), so PMI values are artificially high (8-11 range) for all compounds
2. **PMI favors rare perfect correlations**: "F√∂rderschule" (3 datasets) gets PMI 11.5, while "Schulen" (50 datasets) gets PMI 6.2, but "Schulen" is 10x more useful for search
3. **Frequency = usefulness**: Terms appearing in more datasets are more likely to match user searches
4. **Co-occurrence ratio filters weak associations**: Compound must appear with base ‚â•10% of time to qualify

Example comparison for "verkehr" (traffic):
- PMI ranking: Verkehrserhebungen, Verkehrsmengenkarte, Regelverkehr (rare technical terms)
- Frequency ranking: Stra√üenverkehr, Verkehrsmengen, Radverkehr (common useful terms)

The frequency approach produces expansions users actually search for.

**How expansion generation works (step-by-step):**

The algorithm processes 4,186 candidate words (frequency ‚â• 3) through multiple filtering stages:

**Step 1: Sort by word length (shortest first)**

Candidates sorted by:
1. Word length (shorter words first)
2. Frequency (more common first)

Example order:
```
"rad" (3 chars, freq 50)
"wohn" (4 chars, freq 200)
"wohnen" (6 chars, freq 80)
"fahrrad" (7 chars, freq 17)
"wohngeb√§ude" (11 chars, freq 40)
```

This ensures base words are processed before their longer forms.

**Step 2: Redundancy elimination**

For each word, check if a shorter substring already has an expansion:

Processing "wohnen":
- Does "woh" have an expansion? No
- Does "wohn" have an expansion? **YES** ‚Üí Skip "wohnen" as redundant

Result:
- ‚úì `"wohn": [...]` (kept)
- ‚úó `"wohnen": [...]` (skipped)
- ‚úó `"wohngeb√§ude": [...]` (skipped)

This consolidates word families: 20+ "wohn*" variants ‚Üí 1 entry

**Skipped: 1,304 redundant entries**

**Step 3: Compound word finding**

For each non-redundant base word, scan all 4,186 candidates to find compounds.

A compound must:
- **Contain base word**: "wohngeb√§ude" contains "wohn" ‚úì
- **Be longer**: "wohngeb√§ude" (11) > "wohn" (4) ‚úì
- **Not too long**: ‚â§ 30 characters ‚úì
- **Co-occur enough**: Appear together in ‚â• 2 datasets ‚úì

Example candidates for "wohn":
```
"wohngeb√§udebestand" - 52 datasets
"wohnungen" - 84 datasets
"wohngeb√§ude" - 41 datasets
"wohnraum" - 14 datasets
"nichtwohngeb√§ude" - 8 datasets ‚Üê NEGATION!
```

**Step 4: Co-occurrence ratio filtering**

Calculate for each compound: **co-occurrence ratio = (appears WITH base) / (total appearances)**

Example for "wohngeb√§ude":
- Appears in 41 datasets total
- Appears WITH "wohn" in 40 datasets
- Ratio: 40/41 = 0.976 (97.6%)
- 0.976 ‚â• 0.1 ‚úì **Keep**

Example for weak association:
- "geb√§ude" appears in 200 datasets total
- Appears WITH "wohn" in 10 datasets
- Ratio: 10/200 = 0.05 (5%)
- 0.05 < 0.1 ‚úó **Discard**

This filters compounds that happen to contain the base word but aren't strongly associated.

**Step 5: Negation filtering**

Discard compounds starting with negation prefixes:
- "nichtwohngeb√§ude" starts with "nicht" ‚úó **Discard**
- "unwohnlich" starts with "un" ‚úó **Discard**

Result: "wohn" won't match non-residential buildings

**Step 6: Sort by frequency**

Remaining compounds sorted by dataset frequency (most common first):
```
"wohnungen" - 84 datasets
"wohngeb√§udebestand" - 52 datasets
"wohngeb√§ude" - 41 datasets
"wohnraum" - 14 datasets
"wohnlage" - 8 datasets
```

Take top 5 most frequent.

**Step 7: Quality thresholds**

Only create expansion if:
- **At least 2 distinct terms**: Must have base + at least 1 compound
- **Base word NOT included**: Don't add "Wohn" to its own expansion (redundant)

Example outcomes:
```
‚úì "wohn" ‚Üí 5 compounds found ‚Üí CREATE EXPANSION
‚úó "xyz" ‚Üí 0 compounds found ‚Üí SKIP
‚úó "geodaten" ‚Üí only itself ‚Üí SKIP (would be self-only waste)
```

**Skipped: 2,616 entries with insufficient expansion**

**Final result for "wohn":**
```typescript
"wohn": [
  "Wohngeb√§udebestand",           // freq 52, co-occur 98%
  "Wohn- und nichtwohngeb√§ude",   // freq 45, co-occur 95%
  "Neue wohnungen im wohnbau"     // freq 35, co-occur 92%
]
```

**Summary:**
- Start: 4,186 candidate words (frequency ‚â• 3)
- After redundancy elimination: 2,882 words (skipped 1,304)
- After compound finding + filtering: **266 expansions created** (skipped 2,616)

**Regenerating expansions:**
```bash
npm run generate-expansions  # Re-analyzes portal, updates src/generated-expansions.ts
npm run build                 # Rebuild with new expansions
```

Run when:
- Portal adds significant new datasets with new terminology
- User reports search term not working despite being in portal
- Every 6-12 months to capture evolving vocabulary

**Word statistics:**

To inspect all words extracted from the portal:
```bash
npm run tsx scripts/dump-word-stats.ts  # Generates word-stats-all.json and word-stats-candidates.json
```

This creates:
- `word-stats-all.json` - All 9,913 unique words with frequency and dataset counts
- `word-stats-candidates.json` - 4,186 candidate words (frequency ‚â• 3)

See `ISSUES.md` Issue #3 for complete technical documentation.

---

## Prerequisites

**Required knowledge:**
- TypeScript/Node.js development
- REST API integration
- Model Context Protocol (MCP) basics
- CSV/JSON data parsing

**Tools & libraries already in use:**
- `@modelcontextprotocol/sdk` - MCP server framework
- `node-fetch` - HTTP requests
- `typescript` - Type safety
- `tsx` - Development runtime

**New libraries to add:**
- `papaparse` - Robust CSV parsing with edge case handling
- `@types/papaparse` - TypeScript definitions for papaparse

**Existing codebase structure:**
```
src/
‚îú‚îÄ‚îÄ index.ts              # MCP server setup, tool handlers
‚îú‚îÄ‚îÄ berlin-api.ts         # CKAN API integration
‚îú‚îÄ‚îÄ query-processor.ts    # Natural language query processing
‚îî‚îÄ‚îÄ types.ts             # TypeScript type definitions
```

**Initial setup requirements:**
- Initialize git repository before starting implementation
- Configure .gitignore for Node.js projects

## Design Principles

1. **DRY (Don't Repeat Yourself)**: Extract common patterns into reusable functions
2. **YAGNI (You Aren't Gonna Need It)**: Build only what's specified, no premature optimization
3. **TDD (Test-Driven Development)**: Write tests first when practical, always test after
4. **Frequent commits**: Commit after each completed task with descriptive messages
5. **Keep it simple**: Prefer readable code over clever solutions

---

## Phase 0: Project Setup

**Goal**: Initialize git repository and prepare for implementation.

### Task 0.1: Initialize Git Repository

**What to do**: Create git repository with appropriate .gitignore.

**Step-by-step**:

1. **Initialize git**:
```bash
git init
```

2. **Create .gitignore** (if not already present):
```
node_modules/
dist/
.env
*.log
.DS_Store
```

3. **Initial commit**:
```bash
git add .
git commit -m "Initial commit - Existing search functionality"
```

**Commit message**: `Initial commit - Existing search functionality`

---

### Task 0.2: Install Dependencies

**What to do**: Add papaparse library for robust CSV parsing.

**Step-by-step**:

1. **Install papaparse**:
```bash
npm install papaparse
npm install --save-dev @types/papaparse
```

2. **Verify installation**:
```bash
npm run build
```

**Commit message**: `Add papaparse library for CSV parsing`

---

## Phase 1: Portal Metadata & Navigation

**Goal**: Enable users to understand the overall landscape of available datasets without searching.

**IMPORTANT**: Category and organization filtering tools are SKIPPED. The `search_berlin_datasets` tool already handles these use cases more effectively (e.g., searching for "verkehr" finds 701 datasets vs. 2 with exact tag filtering). Only implement portal stats and list all datasets.

### Task 1.1: Add Portal Statistics API Method

**What to do**: Add a method to fetch high-level portal statistics.

**Files to modify**:
- `src/types.ts` - Add new interface
- `src/berlin-api.ts` - Add new method

**Step-by-step**:

1. **Add type definition** in `src/types.ts`:
```typescript
export interface PortalStats {
  total_datasets: number;
  total_organizations: number;
  total_tags: number;
  last_updated?: string;
}
```

2. **Add method to BerlinOpenDataAPI** in `src/berlin-api.ts`:
```typescript
async getPortalStats(): Promise<PortalStats> {
  // CKAN doesn't have a dedicated stats endpoint, so we aggregate
  const [datasets, orgs, tags] = await Promise.all([
    this.makeRequest('package_search', { rows: 0 }), // Just get count
    this.listOrganizations(),
    this.listTags(0), // Get all tags for count
  ]);

  return {
    total_datasets: datasets.count,
    total_organizations: orgs.length,
    total_tags: tags.length,
  };
}
```

**How to test**:
```bash
# In development mode
npm run dev

# In another terminal, test the API directly
node -e "
const { BerlinOpenDataAPI } = require('./dist/berlin-api.js');
const api = new BerlinOpenDataAPI();
api.getPortalStats().then(console.log);
"
```

**Expected output**: Object with counts for datasets, organizations, and tags.

**Commit message**: `Add getPortalStats method to BerlinOpenDataAPI`

---

### Task 1.2: Add List All Datasets with Pagination

**What to do**: Extend existing `listDatasets` method to support pagination properly.

**Files to modify**:
- `src/berlin-api.ts` - Update existing method

**Step-by-step**:

1. **Update listDatasets method** in `src/berlin-api.ts`:

Replace the existing method (around line 63) with:
```typescript
async listAllDatasets(offset: number = 0, limit: number = 100): Promise<{ datasets: string[]; total: number }> {
  // Use package_search instead of package_list for better pagination
  const result = await this.makeRequest('package_search', {
    q: '*:*', // Match all
    rows: limit,
    start: offset,
    fl: 'name,title', // Only fetch name and title fields
  });

  return {
    datasets: result.results.map((d: any) => ({ name: d.name, title: d.title })),
    total: result.count,
  };
}
```

**Why this approach**: `package_search` with `q: '*:*'` gives us proper pagination and counts, unlike `package_list` which has limitations.

**How to test**:
```bash
npm run build
node -e "
const { BerlinOpenDataAPI } = require('./dist/berlin-api.js');
const api = new BerlinOpenDataAPI();

// Test pagination
api.listAllDatasets(0, 10).then(result => {
  console.log('First 10 datasets:', result.datasets.length);
  console.log('Total:', result.total);

  // Test offset
  return api.listAllDatasets(10, 10);
}).then(result => {
  console.log('Next 10 datasets:', result.datasets.length);
});
"
```

**Expected output**: Two lists of 10 datasets each, plus total count.

**Commit message**: `Update listAllDatasets with proper pagination support`

---

### Task 1.3: Add MCP Tools for Portal Metadata

**What to do**: Expose the new API methods as MCP tools that Claude can use.

**Files to modify**:
- `src/index.ts` - Add new tool definitions and handlers

**Step-by-step**:

1. **Add tool definitions** in the `ListToolsRequestSchema` handler (around line 40):

Add these to the `tools` array:
```typescript
{
  name: 'get_portal_stats',
  description: 'Get overview statistics about the Berlin Open Data Portal (total datasets, organizations, categories)',
  inputSchema: {
    type: 'object',
    properties: {},
  },
},
{
  name: 'list_all_datasets',
  description: 'List all datasets in the portal with pagination support. Use this to browse the entire catalog.',
  inputSchema: {
    type: 'object',
    properties: {
      offset: {
        type: 'number',
        description: 'Starting position (default: 0)',
        default: 0,
      },
      limit: {
        type: 'number',
        description: 'Number of results to return (default: 100, max: 1000)',
        default: 100,
      },
    },
  },
},
```

**Note**: DO NOT implement category and organization filtering tools. The `search_berlin_datasets` tool handles these use cases more effectively.

2. **Add tool handlers** in the `CallToolRequestSchema` handler (around line 119):

Add these cases to the switch statement:
```typescript
case 'get_portal_stats': {
  const stats = await this.api.getPortalStats();

  let responseText = '# Berlin Open Data Portal Statistics\n\n';
  responseText += `üìä **Total Datasets**: ${stats.total_datasets}\n`;
  responseText += `üèõÔ∏è **Organizations**: ${stats.total_organizations}\n`;
  responseText += `üè∑Ô∏è **Categories/Tags**: ${stats.total_tags}\n`;

  responseText += '\nüí° **Next steps**:\n';
  responseText += '- Use `list_all_datasets` to browse all datasets\n';
  responseText += '- Use `search_berlin_datasets` to find specific topics\n';

  return {
    content: [{ type: 'text', text: responseText }],
  };
}

case 'list_all_datasets': {
  const { offset = 0, limit = 100 } = args as { offset?: number; limit?: number };
  const result = await this.api.listAllDatasets(offset, limit);

  let responseText = `# All Berlin Open Datasets\n\n`;
  responseText += `Showing ${offset + 1}-${Math.min(offset + limit, result.total)} of ${result.total} datasets\n\n`;

  result.datasets.forEach((dataset: any, index: number) => {
    responseText += `${offset + index + 1}. **${dataset.title}** (ID: ${dataset.name})\n`;
  });

  if (offset + limit < result.total) {
    responseText += `\nüìÑ **More data available**: Use offset=${offset + limit} to see next page\n`;
  }

  responseText += `\nüí° Use \`get_dataset_details\` with any ID to see full information\n`;

  return {
    content: [{ type: 'text', text: responseText }],
  };
}

```

**How to test**:
You'll need to test this with an actual MCP client (like Claude Desktop). For now, verify the code compiles:

```bash
npm run build
```

Check for TypeScript errors. If it compiles successfully, you're good.

**Commit message**: `Add MCP tools for portal stats and dataset listing`

---

### Phase 1 Complete!

**Final checkpoint**:
1. Build the project: `npm run build`
2. Verify no TypeScript errors
3. Test API methods directly (getPortalStats, listAllDatasets)
4. Review your commits - should have 3 commits for Phase 1

---

## Phase 2: Data Fetching & Sampling

**Goal**: Enable users to fetch actual dataset contents (CSV/JSON) with smart sampling to avoid context overflow.

### Task 2.1: Create Data Fetcher Module

**What to do**: Create a new module that downloads and parses dataset resources.

**Files to create**:
- `src/data-fetcher.ts` - New file

**Step-by-step**:

1. **Create the file** `src/data-fetcher.ts`:

```typescript
// ABOUTME: Downloads and parses dataset resources from URLs
// ABOUTME: Handles CSV and JSON formats with robust error handling

import fetch from 'node-fetch';
import Papa from 'papaparse';

export interface FetchedData {
  format: string;
  rows: any[];
  totalRows: number;
  columns: string[];
  error?: string;
}

export class DataFetcher {
  private readonly MAX_DOWNLOAD_SIZE = 50 * 1024 * 1024; // 50MB limit
  private readonly REQUEST_TIMEOUT = 30000; // 30 seconds

  async fetchResource(url: string, format: string): Promise<FetchedData> {
    try {
      // Download the resource
      const response = await fetch(url, {
        timeout: this.REQUEST_TIMEOUT,
        headers: {
          'User-Agent': 'Berlin-Open-Data-MCP-Server/1.0',
        },
      });

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`);
      }

      // Check content length
      const contentLength = response.headers.get('content-length');
      if (contentLength && parseInt(contentLength) > this.MAX_DOWNLOAD_SIZE) {
        throw new Error(`File too large: ${contentLength} bytes (max: ${this.MAX_DOWNLOAD_SIZE})`);
      }

      const contentType = response.headers.get('content-type') || '';
      const text = await response.text();

      // Parse based on format
      return this.parseData(text, format, contentType);
    } catch (error) {
      return {
        format,
        rows: [],
        totalRows: 0,
        columns: [],
        error: error instanceof Error ? error.message : String(error),
      };
    }
  }

  private parseData(text: string, format: string, contentType: string): FetchedData {
    const formatLower = format.toLowerCase();

    // Try JSON first if format or content-type suggests it
    if (formatLower.includes('json') || contentType.includes('json')) {
      return this.parseJSON(text, format);
    }

    // Try CSV - use papaparse for robust parsing
    if (formatLower.includes('csv') || contentType.includes('csv') || contentType.includes('text')) {
      return this.parseCSV(text, format);
    }

    // Default to CSV parsing
    return this.parseCSV(text, format);
  }

  private parseJSON(text: string, format: string): FetchedData {
    try {
      const parsed = JSON.parse(text);

      // Handle different JSON structures
      let rows: any[];
      if (Array.isArray(parsed)) {
        rows = parsed;
      } else if (parsed.data && Array.isArray(parsed.data)) {
        rows = parsed.data;
      } else if (parsed.results && Array.isArray(parsed.results)) {
        rows = parsed.results;
      } else if (typeof parsed === 'object') {
        // Single object - wrap in array
        rows = [parsed];
      } else {
        throw new Error('Unexpected JSON structure');
      }

      // Extract columns from first row
      const columns = rows.length > 0 ? Object.keys(rows[0]) : [];

      return {
        format: 'JSON',
        rows,
        totalRows: rows.length,
        columns,
      };
    } catch (error) {
      return {
        format,
        rows: [],
        totalRows: 0,
        columns: [],
        error: `JSON parse error: ${error instanceof Error ? error.message : String(error)}`,
      };
    }
  }

  private parseCSV(text: string, format: string): FetchedData {
    try {
      // Use papaparse for robust CSV parsing
      // Automatically detects delimiters, handles quotes, encoding issues, etc.
      const result = Papa.parse(text, {
        header: true,
        skipEmptyLines: true,
        dynamicTyping: false, // Keep all as strings, we'll infer types later
        encoding: 'utf-8',
      });

      if (result.errors.length > 0) {
        console.warn('CSV parsing warnings:', result.errors);
      }

      const rows = result.data as any[];
      const columns = result.meta.fields || [];

      return {
        format: 'CSV',
        rows,
        totalRows: rows.length,
        columns,
      };
    } catch (error) {
      return {
        format,
        rows: [],
        totalRows: 0,
        columns: [],
        error: `CSV parse error: ${error instanceof Error ? error.message : String(error)}`,
      };
    }
  }
}
```

**How to test**:

Create a test script `test-fetcher.js`:
```javascript
const { DataFetcher } = require('./dist/data-fetcher.js');

const fetcher = new DataFetcher();

// Test with a known Berlin dataset URL (find one from the portal)
// Example: A small CSV file
const testUrl = 'https://www.berlin.de/sen/sbw/_assets/verkehr/daten-und-fakten/zahlen-und-fakten/verkehrsmengen/vz2019.csv';

fetcher.fetchResource(testUrl, 'CSV').then(result => {
  console.log('Format:', result.format);
  console.log('Total rows:', result.totalRows);
  console.log('Columns:', result.columns);
  console.log('First row:', result.rows[0]);
  console.log('Error:', result.error);
});
```

```bash
npm run build
node test-fetcher.js
```

**Expected output**: Parsed CSV data with columns and rows.

**Commit message**: `Add DataFetcher module for downloading and parsing resources`

---

### Task 2.2: Create Data Sampler Module

**What to do**: Create a module that generates minimal previews from fetched data.

**Files to create**:
- `src/data-sampler.ts` - New file

**Step-by-step**:

1. **Create the file** `src/data-sampler.ts`:

```typescript
// ABOUTME: Generates minimal data previews from dataset rows
// ABOUTME: Returns 10-row samples with basic column type inference

export interface ColumnStats {
  name: string;
  type: 'number' | 'string' | 'boolean' | 'date' | 'unknown';
}

export interface DataSample {
  sampleRows: any[];
  totalRows: number;
  isTruncated: boolean;
  columns: ColumnStats[];
  summary: string;
}

export class DataSampler {
  private readonly DEFAULT_SAMPLE_SIZE = 10;

  generateSample(rows: any[], columns: string[]): DataSample {
    const sampleRows = rows.slice(0, this.DEFAULT_SAMPLE_SIZE);

    // Generate minimal column statistics (name and type only)
    const columnStats = columns.map(colName => this.analyzeColumn(colName, rows));

    // Generate summary text
    const summary = this.generateSummary(rows.length, columns.length, columnStats);

    return {
      sampleRows,
      totalRows: rows.length,
      isTruncated: rows.length > this.DEFAULT_SAMPLE_SIZE,
      columns: columnStats,
      summary,
    };
  }

  private analyzeColumn(columnName: string, rows: any[]): ColumnStats {
    const values = rows.map(row => row[columnName]);
    const nonNullValues = values.filter(v => v != null && v !== '');

    // Infer type
    const type = this.inferType(nonNullValues);

    return {
      name: columnName,
      type,
    };
  }

  private inferType(values: any[]): 'number' | 'string' | 'boolean' | 'date' | 'unknown' {
    if (values.length === 0) return 'unknown';

    // Sample first 100 non-null values
    const sample = values.slice(0, 100);

    // Check if all are numbers
    const numericCount = sample.filter(v => !isNaN(parseFloat(v)) && isFinite(v)).length;
    if (numericCount / sample.length > 0.8) return 'number';

    // Check if all are booleans
    const boolCount = sample.filter(v =>
      v === true || v === false || v === 'true' || v === 'false' || v === '0' || v === '1'
    ).length;
    if (boolCount / sample.length > 0.8) return 'boolean';

    // Check if looks like dates
    const dateCount = sample.filter(v => {
      const str = String(v);
      return /^\d{4}-\d{2}-\d{2}/.test(str) || /^\d{2}\/\d{2}\/\d{4}/.test(str);
    }).length;
    if (dateCount / sample.length > 0.8) return 'date';

    return 'string';
  }

  private generateSummary(totalRows: number, totalColumns: number, columns: ColumnStats[]): string {
    let summary = `Dataset contains ${totalRows} rows and ${totalColumns} columns.\n\n`;
    summary += '**Columns:**\n';
    summary += columns.map(col => `- ${col.name} (${col.type})`).join('\n');
    return summary;
  }
}
```

**How to test**:

Create a test script `test-sampler.js`:
```javascript
const { DataSampler } = require('./dist/data-sampler.js');

const sampler = new DataSampler();

// Mock data
const testData = [
  { name: 'Alice', age: 30, city: 'Berlin', score: 95.5 },
  { name: 'Bob', age: 25, city: 'Munich', score: 87.3 },
  { name: 'Charlie', age: 35, city: 'Berlin', score: 92.1 },
  { name: 'David', age: 28, city: 'Hamburg', score: null },
];

const columns = ['name', 'age', 'city', 'score'];

const sample = sampler.generateSample(testData, columns);

console.log('Summary:\n', sample.summary);
console.log('\nColumn stats:', JSON.stringify(sample.columns, null, 2));
console.log('\nSample rows:', sample.sampleRows);
```

```bash
npm run build
node test-sampler.js
```

**Expected output**: Summary text and column statistics showing inferred types and ranges.

**Commit message**: `Add DataSampler module for smart sampling and statistics`

---

### Task 2.3: Add Resource Listing Method to API

**What to do**: Add a helper method to easily list resources for a dataset.

**Files to modify**:
- `src/berlin-api.ts` - Add new method

**Step-by-step**:

1. **Add method** in `src/berlin-api.ts`:

```typescript
async listDatasetResources(datasetId: string): Promise<Array<{ id: string; name: string; format: string; url: string; description: string }>> {
  const dataset = await this.getDataset(datasetId);

  return dataset.resources.map(r => ({
    id: r.id,
    name: r.name,
    format: r.format,
    url: r.url,
    description: r.description,
  }));
}
```

**How to test**:
```bash
npm run build
node -e "
const { BerlinOpenDataAPI } = require('./dist/berlin-api.js');
const api = new BerlinOpenDataAPI();

// Use a known dataset ID (find one from previous tests)
api.searchDatasets({ query: 'verkehr', limit: 1 }).then(result => {
  const datasetId = result.results[0].name;
  console.log('Testing with dataset:', datasetId);
  return api.listDatasetResources(datasetId);
}).then(resources => {
  console.log('Resources found:', resources.length);
  resources.forEach(r => {
    console.log('-', r.name, '(' + r.format + ')');
  });
});
"
```

**Expected output**: List of resources with their formats and URLs.

**Commit message**: `Add listDatasetResources helper method`

---

### Task 2.4: Integrate Fetcher and Sampler into Main Server

**What to do**: Wire up the data fetching and sampling into the MCP server.

**Files to modify**:
- `src/index.ts` - Import and instantiate new modules

**Step-by-step**:

1. **Add imports** at the top of `src/index.ts`:
```typescript
import { DataFetcher } from './data-fetcher.js';
import { DataSampler } from './data-sampler.js';
```

2. **Add instance variables** to the `BerlinOpenDataMCPServer` class (around line 16):
```typescript
private dataFetcher: DataFetcher;
private dataSampler: DataSampler;
```

3. **Initialize in constructor** (around line 34):
```typescript
this.dataFetcher = new DataFetcher();
this.dataSampler = new DataSampler();
```

**How to test**:
```bash
npm run build
```

Should compile without errors.

**Commit message**: `Integrate DataFetcher and DataSampler into MCP server`

---

### Task 2.5: Add MCP Tools for Data Fetching

**What to do**: Add MCP tool definitions for fetching dataset data.

**Files to modify**:
- `src/index.ts` - Add new tools

**Step-by-step**:

1. **Add tool definition** in `ListToolsRequestSchema` handler:

```typescript
{
  name: 'fetch_dataset_data',
  description: 'Download and parse Berlin Open Data datasets. Returns 10 sample rows initially. For small datasets (‚â§500 rows), use full_data: true to get all data. Large datasets must be downloaded manually.',
  inputSchema: {
    type: 'object',
    properties: {
      dataset_id: {
        type: 'string',
        description: 'The dataset ID or name',
      },
      resource_id: {
        type: 'string',
        description: 'Optional: specific resource ID from get_dataset_details. If not provided, uses first available resource.',
      },
      full_data: {
        type: 'boolean',
        description: 'If true, return all data for small datasets (‚â§500 rows). Refused for large datasets.',
        default: false,
      },
    },
    required: ['dataset_id'],
  },
},
```

2. **Enhance get_dataset_details** to include resource IDs in output:

```typescript
dataset.resources.forEach((resource, index) => {
  details += `### ${index + 1}. ${resource.name || 'Unnamed Resource'}\n`;
  if (resource.id) {
    details += `**Resource ID**: ${resource.id}\n`;  // ADD THIS
  }
  // ... rest of resource info
});
```

3. **Add tool handler** in `CallToolRequestSchema` handler:

```typescript
case 'fetch_dataset_data': {
  const { dataset_id, resource_id, full_data = false } = args as {
    dataset_id: string;
    resource_id?: string;
    full_data?: boolean;
  };

  const LARGE_DATASET_THRESHOLD = 500;

  // Get dataset to find resources
  const dataset = await this.api.getDataset(dataset_id);

  if (!dataset.resources || dataset.resources.length === 0) {
    return {
      content: [{
        type: 'text',
        text: `‚ùå No resources available for dataset "${dataset_id}". This dataset may not have downloadable files.`,
      }],
    };
  }

  // Select resource
  let resource;
  if (resource_id) {
    resource = dataset.resources.find(r => r.id === resource_id);
    if (!resource) {
      return {
        content: [{
          type: 'text',
          text: `‚ùå Resource "${resource_id}" not found. Use \`get_dataset_details\` to see available resources and their IDs.`,
        }],
      };
    }
  } else {
    // Use first resource
    resource = dataset.resources[0];
  }

  // Fetch the data
  const fetchedData = await this.dataFetcher.fetchResource(resource.url, resource.format);

  if (fetchedData.error) {
    return {
      content: [{
        type: 'text',
        text: `‚ùå Error fetching data: ${fetchedData.error}\n\nYou can try:\n- Using a different resource\n- Downloading manually from: ${resource.url}`,
      }],
    };
  }

  const totalRows = fetchedData.rows.length;
  const isLarge = totalRows > LARGE_DATASET_THRESHOLD;
  const sizeLabel = isLarge ? 'large' : 'small';

  let responseText = `# Data from: ${dataset.title}\n\n`;
  responseText += `**Resource**: ${resource.name} (${resource.format})\n\n`;

  // Handle full_data request for large datasets
  if (full_data && isLarge) {
    return {
      content: [{
        type: 'text',
        text: `‚ùå Dataset has ${totalRows} rows and is too large for direct analysis. Returning all data would risk context overflow.\n\nüì• **Download manually**: ${resource.url}\n\nOnce downloaded, attach the file to Claude Desktop for analysis.`,
      }],
    };
  }

  // Return full data for small datasets when requested
  if (full_data) {
    responseText += `Dataset has ${totalRows} rows. This is a **${sizeLabel} dataset**.\n\n`;
    responseText += `## Full Dataset\n\n`;
    responseText += `**Data:**\n\`\`\`json\n${JSON.stringify(fetchedData.rows, null, 2)}\n\`\`\`\n`;
    return {
      content: [{ type: 'text', text: responseText }],
    };
  }

  // Always return 10-row sample initially
  const sample = this.dataSampler.generateSample(
    fetchedData.rows,
    fetchedData.columns
  );

  responseText += `Dataset has ${totalRows} rows. This is a **${sizeLabel} dataset**.\n\n`;
  responseText += `## Data Sample\n\n`;
  responseText += sample.summary + '\n';
  responseText += `\n**Sample Data (first ${sample.sampleRows.length} rows):**\n`;
  responseText += `\`\`\`json\n${JSON.stringify(sample.sampleRows, null, 2)}\n\`\`\`\n\n`;

  if (isLarge) {
    responseText += `‚ö†Ô∏è **For complete analysis**: Download from ${resource.url} and attach to Claude Desktop.\n`;
  } else {
    responseText += `üí° Use \`full_data: true\` to analyze all ${totalRows} rows.\n`;
  }

  return {
    content: [{ type: 'text', text: responseText }],
  };
}
```

**How to test**:
```bash
npm run build
```

Verify compilation succeeds.

**Commit message**: `Add MCP tools for listing and fetching dataset resources`

---

### Task 2.6: Add Error Handling for Edge Cases

**What to do**: Improve error handling for common failure scenarios.

**Files to modify**:
- `src/data-fetcher.ts` - Enhance error messages

**Step-by-step**:

1. **Update error handling** in `DataFetcher.fetchResource`:

Replace the catch block (around line 30) with:
```typescript
catch (error) {
  let errorMessage = 'Unknown error';

  if (error instanceof Error) {
    if (error.message.includes('timeout')) {
      errorMessage = 'Download timeout - file may be too large or server is slow';
    } else if (error.message.includes('ENOTFOUND') || error.message.includes('ECONNREFUSED')) {
      errorMessage = 'Could not connect to server - URL may be invalid';
    } else if (error.message.includes('Too large')) {
      errorMessage = error.message;
    } else {
      errorMessage = error.message;
    }
  }

  return {
    format,
    rows: [],
    totalRows: 0,
    columns: [],
    error: errorMessage,
  };
}
```

**How to test**:
Test with an invalid URL:
```bash
npm run build
node -e "
const { DataFetcher } = require('./dist/data-fetcher.js');
const fetcher = new DataFetcher();

fetcher.fetchResource('https://invalid-url-12345.com/data.csv', 'CSV').then(result => {
  console.log('Error message:', result.error);
});
"
```

**Expected output**: User-friendly error message about connection failure.

**Commit message**: `Improve error handling in DataFetcher`

---

### Phase 2 Complete!

**Final checkpoint**:
1. Build: `npm run build`
2. Test data fetcher with a real dataset URL
3. Test data sampler with mock data
4. Verify TypeScript compilation succeeds
5. Count commits - should have 6-7 for Phase 2

---

## Phase 3: Testing & Refinement

**Goal**: End-to-end integration testing with real usage scenarios and documentation.

**IMPORTANT**: Focus on integration tests first. Manual testing with Claude Desktop is secondary and can be deferred.

### Task 3.1: Create Integration Test Script

**What to do**: Create a comprehensive test that exercises all tools.

**Files to create**:
- `test/integration-test.js` - New file

**Step-by-step**:

1. **Create test directory**:
```bash
mkdir -p test
```

2. **Create `test/integration-test.js`**:

```javascript
// Integration test for Berlin Open Data MCP Server
// Tests all major workflows

const { BerlinOpenDataAPI } = require('../dist/berlin-api.js');
const { DataFetcher } = require('../dist/data-fetcher.js');
const { DataSampler } = require('../dist/data-sampler.js');

async function runTests() {
  console.log('üß™ Starting Integration Tests\n');

  const api = new BerlinOpenDataAPI();
  const fetcher = new DataFetcher();
  const sampler = new DataSampler();

  let passed = 0;
  let failed = 0;

  // Test 1: Portal Stats
  try {
    console.log('Test 1: Get portal statistics...');
    const stats = await api.getPortalStats();
    console.assert(stats.total_datasets > 0, 'Should have datasets');
    console.assert(stats.total_organizations > 0, 'Should have organizations');
    console.log('‚úÖ Portal stats:', stats);
    passed++;
  } catch (error) {
    console.log('‚ùå Failed:', error.message);
    failed++;
  }

  // Test 2: List datasets with pagination
  try {
    console.log('\nTest 2: List datasets with pagination...');
    const result = await api.listAllDatasets(0, 10);
    console.assert(result.datasets.length === 10, 'Should return 10 datasets');
    console.assert(result.total > 10, 'Total should be greater than 10');
    console.log('‚úÖ Listed datasets:', result.datasets.length, 'of', result.total);
    passed++;
  } catch (error) {
    console.log('‚ùå Failed:', error.message);
    failed++;
  }

  // Test 3: Search datasets
  try {
    console.log('\nTest 3: Search datasets...');
    const results = await api.searchDatasets({ query: 'verkehr', limit: 5 });
    console.assert(results.results.length > 0, 'Should find results');
    console.log('‚úÖ Found', results.results.length, 'datasets about verkehr');

    // Save first dataset for next tests
    global.testDatasetId = results.results[0].name;
    global.testDatasetTitle = results.results[0].title;
  } catch (error) {
    console.log('‚ùå Failed:', error.message);
    failed++;
  }

  // Test 4: Get dataset details
  try {
    console.log('\nTest 4: Get dataset details...');
    const dataset = await api.getDataset(global.testDatasetId);
    console.assert(dataset.id, 'Dataset should have ID');
    console.assert(dataset.resources, 'Dataset should have resources');
    console.log('‚úÖ Got details for:', dataset.title);
    console.log('   Resources:', dataset.resources.length);
    passed++;
  } catch (error) {
    console.log('‚ùå Failed:', error.message);
    failed++;
  }

  // Test 5: List dataset resources
  try {
    console.log('\nTest 5: List dataset resources...');
    const resources = await api.listDatasetResources(global.testDatasetId);
    console.assert(Array.isArray(resources), 'Should return array');
    console.log('‚úÖ Found', resources.length, 'resources');

    if (resources.length > 0) {
      global.testResourceUrl = resources[0].url;
      global.testResourceFormat = resources[0].format;
    }
    passed++;
  } catch (error) {
    console.log('‚ùå Failed:', error.message);
    failed++;
  }

  // Test 6: Fetch and parse data (if we have a suitable resource)
  if (global.testResourceUrl && ['CSV', 'JSON'].includes(global.testResourceFormat.toUpperCase())) {
    try {
      console.log('\nTest 6: Fetch and parse data...');
      const data = await fetcher.fetchResource(global.testResourceUrl, global.testResourceFormat);

      if (data.error) {
        console.log('‚ö†Ô∏è  Could not fetch data:', data.error);
      } else {
        console.assert(data.rows.length > 0, 'Should have rows');
        console.assert(data.columns.length > 0, 'Should have columns');
        console.log('‚úÖ Fetched data:', data.rows.length, 'rows,', data.columns.length, 'columns');

        // Test 7: Generate sample
        console.log('\nTest 7: Generate sample and stats...');
        const sample = sampler.generateSample(data.rows, data.columns, 10);
        console.assert(sample.sampleRows.length <= 10, 'Sample should be limited');
        console.assert(sample.columns.length > 0, 'Should have column stats');
        console.log('‚úÖ Generated sample with stats');
        console.log('   Column types:', sample.columns.map(c => `${c.name}:${c.type}`).join(', '));
        passed += 2;
      }
    } catch (error) {
      console.log('‚ùå Failed:', error.message);
      failed += 2;
    }
  } else {
    console.log('\n‚è≠Ô∏è  Skipping data fetch tests (no suitable resource)');
  }

  // Summary
  console.log('\n' + '='.repeat(50));
  console.log(`Tests completed: ${passed} passed, ${failed} failed`);

  if (failed === 0) {
    console.log('‚úÖ All tests passed!');
    process.exit(0);
  } else {
    console.log('‚ùå Some tests failed');
    process.exit(1);
  }
}

runTests();
```

**How to test**:
```bash
npm run build
node test/integration-test.js
```

**Expected output**: Should see all tests pass with real data from the portal.

**Commit message**: `Add integration test script for all features`

---

### Task 3.2: Update README with New Features

**What to do**: Document the new capabilities in the README.

**Files to modify**:
- `README.md` - Update features and tools sections

**Step-by-step**:

1. **Update the Features section** (around line 5):

Replace the existing features with:
```markdown
## Features

- üîç **Natural Language Search**: Query datasets using plain English
- üìä **Dataset Discovery**: Browse datasets by category, organization, or explore all available data
- üìà **Portal Overview**: Get statistics and understand the data landscape
- üíæ **Data Fetching**: Download and parse dataset contents (CSV, JSON)
- üéØ **Smart Sampling**: Automatic data sampling with statistics to prevent context overflow
- üîó **Direct API Integration**: Connects to Berlin's official CKAN-based data portal
- ü§ñ **Agentic Workflows**: Tools can be chained together for complex analysis tasks
```

2. **Update the Tools section** (around line 22):

Replace with:
```markdown
### Tools

**Portal Metadata & Navigation:**
1. **get_portal_stats**: Get overview statistics (total datasets, organizations, categories)
2. **list_all_datasets**: Browse all datasets with pagination

**Dataset Discovery:**
3. **search_berlin_datasets**: Search datasets using natural language
4. **get_dataset_details**: Get detailed information about a specific dataset

**Data Fetching & Analysis:**
5. **fetch_dataset_data**: Download and parse dataset contents with smart sampling
```

3. **Add a new Examples section** after the Tools section:

```markdown
### Workflow Examples

**Explore the portal:**
```
User: "What's available in the Berlin Open Data Portal?"
‚Üí Uses get_portal_stats
‚Üí Gets overview with counts and suggestions
```

**Find and analyze data:**
```
User: "Which Berlin district has the most green space per capita?"
‚Üí Uses search_berlin_datasets for green space data
‚Üí Uses fetch_dataset_data to get the actual data
‚Üí Performs calculation using fetched data
‚Üí Returns answer with methodology
```

**Multi-dataset analysis:**
```
User: "Is there correlation between air quality and traffic?"
‚Üí Searches for air quality datasets
‚Üí Searches for traffic datasets
‚Üí Fetches both datasets
‚Üí Analyzes correlation
‚Üí Returns findings
```
```

**How to test**:
Read through the updated README to ensure it's accurate and clear.

**Commit message**: `Update README with Phase 1 and 2 features`

---

### Task 3.3: Add Usage Examples Document

**What to do**: Create detailed usage examples for developers/users.

**Files to create**:
- `docs/USAGE_EXAMPLES.md` - New file

**Step-by-step**:

1. **Create `docs/USAGE_EXAMPLES.md`**:

```markdown
# Usage Examples

This document shows real-world usage examples of the Berlin Open Data MCP Server.

## Getting Started

### Basic Portal Exploration

**Question**: "What data is available in Berlin?"

**Tools used**: `get_portal_stats`

**Result**: Overview showing:
- Total datasets: ~1500
- Organizations: ~50
- Categories: ~200

---

### Finding Specific Data

**Question**: "Find datasets about bicycle infrastructure"

**Tools used**: `search_berlin_datasets`

**Result**: List of relevant datasets including:
- Bicycle parking locations
- Bike lane network data
- Bike-sharing station information

---

---

### Fetching Actual Data

**Question**: "Get the bicycle parking data"

**Tools used**:
1. `search_berlin_datasets` to find the dataset
2. `get_dataset_details` to see available resources
3. `fetch_dataset_data` to download and parse the data

**Result**: Smart sample of the data (first 100 rows) with:
- Column names and types
- Statistics (min/max for numbers, unique counts)
- Sample values
- Total row count

---

## Advanced Workflows

### Multi-Dataset Analysis

**Question**: "Which district has the most parks?"

**Workflow**:
1. Search for park/green space datasets
2. Fetch the green space data
3. Search for district boundary data
4. Fetch district data
5. Perform aggregation by district
6. Return answer

**Tools chain**:
```
search_berlin_datasets("parks green space")
‚Üí get_dataset_details(dataset_id)
‚Üí fetch_dataset_data(dataset_id)
‚Üí [Analysis performed by LLM using fetched data]
‚Üí Answer
```

---

### Correlation Analysis

**Question**: "Is there a relationship between air quality and green spaces?"

**Workflow**:
1. Find air quality measurement data
2. Find green space area data
3. Fetch both datasets
4. Align by district/location
5. Calculate correlation
6. Interpret results

**Tools chain**:
```
search_berlin_datasets("air quality luftqualit√§t")
‚Üí fetch_dataset_data(air_quality_dataset_id)
‚Üí search_berlin_datasets("green space gr√ºnfl√§chen")
‚Üí fetch_dataset_data(green_space_dataset_id)
‚Üí [LLM performs correlation analysis]
‚Üí Answer with statistical findings
```

---

## Tips for Effective Usage

### 1. Start Broad, Then Narrow

```
‚ùå Bad: Immediately fetching data without knowing what's available
‚úÖ Good: get_portal_stats ‚Üí search ‚Üí fetch
```

### 2. Check Resources Before Fetching

```
‚ùå Bad: fetch_dataset_data without knowing the format
‚úÖ Good: get_dataset_details ‚Üí choose appropriate resource ‚Üí fetch
```

### 3. Use Smart Sampling for Large Datasets

```
‚ùå Bad: fetch_dataset_data with full_data=true on 100k row dataset
‚úÖ Good: fetch_dataset_data with default sampling (100 rows)
```

---

## Common Patterns

### Pattern 1: Data Discovery
```
get_portal_stats
‚Üí search_berlin_datasets
‚Üí get_dataset_details
```

### Pattern 2: Quick Analysis
```
search_berlin_datasets
‚Üí fetch_dataset_data
‚Üí [Analysis]
```

### Pattern 3: Comprehensive Study
```
get_portal_stats
‚Üí search_berlin_datasets (with category keywords)
‚Üí [Select multiple datasets]
‚Üí fetch_dataset_data (multiple times)
‚Üí [Cross-dataset analysis]
```

---

## Troubleshooting

### "No resources available"
- Some datasets are metadata-only
- Use `get_dataset_details` to check if resources exist
- Try related datasets instead

### "Error fetching data"
- Resource URL may be invalid or server down
- Try different resource from same dataset
- Check format - only CSV and JSON are fully supported

### "Dataset too large"
- Use smart sampling (default behavior)
- Consider filtering data at source if API supports it
- Analyze sample and extrapolate if appropriate

### "No results found"
- Try German keywords (e.g., "Verkehr" vs "traffic")
- Use broader search terms
- Try `list_all_datasets` to browse the catalog

---

## Performance Tips

1. **Pagination**: Always specify reasonable `limit` values (10-100)
2. **Sample first**: Don't use `full_data=true` unless necessary
3. **Reuse dataset IDs**: If analyzing multiple resources from same dataset, store the ID

---

## API Limits

- Maximum download size: 50MB per resource
- Default sample size: 100 rows
- Maximum sample size: 1000 rows
- Request timeout: 30 seconds
- No rate limiting (but be respectful)
```

**How to test**:
Review the document for accuracy and completeness.

**Commit message**: `Add comprehensive usage examples documentation`

---

### Task 3.4: Fix Any Bugs Found

**What to do**: Address any issues discovered during testing.

**Process**:

1. For each bug:
   - Document the issue in the test log
   - Identify root cause
   - Fix in appropriate file
   - Test the fix
   - Commit with descriptive message: `Fix: [description of bug]`

2. Common potential issues to watch for:
   - Empty dataset results (handle gracefully)
   - Invalid dataset IDs (provide helpful error)
   - Missing resources (suggest alternatives)
   - Large file downloads (timeout handling)
   - JSON parsing errors (malformed data)

**Note**: CSV parsing edge cases (delimiters, encoding) are handled by papaparse library.

---

### Task 3.5: Update CHANGELOG

**What to do**: Document all changes made in Phases 1-3.

**Files to create**:
- `CHANGELOG.md` - New file

**Step-by-step**:

1. **Create `CHANGELOG.md`**:

```markdown
# Changelog

All notable changes to the Berlin Open Data MCP Server.

## [2.0.0] - [Date]

### Added - Phase 1: Portal Metadata & Navigation
- `get_portal_stats` tool for portal overview
- `list_all_datasets` tool with proper pagination
- Portal statistics API methods
- Enhanced pagination support

### Added - Phase 2: Data Fetching & Sampling
- Enhanced `get_dataset_details` to include resource IDs
- `fetch_dataset_data` tool to download and parse data
- DataFetcher module for downloading CSV/JSON resources with papaparse
- DataSampler module for smart sampling and statistics
- Automatic format detection and conversion
- Column type inference and statistics
- Sample size limits to prevent context overflow
- Support for CSV and JSON formats with robust parsing

### Added - Phase 3: Documentation & Testing
- Integration test suite (prioritized over manual testing)
- Comprehensive usage examples
- Updated README with new features

### Improved
- Error handling for network failures
- Error messages with actionable suggestions
- Type safety throughout codebase

## [1.0.0] - [Original date]

### Initial Release
- Basic dataset search functionality
- Dataset details retrieval
- Category and organization listing
- Natural language query processing
- MCP protocol integration
```

**Commit message**: `Add CHANGELOG for version 2.0.0`

---

### Phase 3 Complete!

**Final verification checklist**:

- [ ] All code compiles without errors
- [ ] Integration tests pass
- [ ] Manual testing completed
- [ ] Documentation updated (README, usage examples)
- [ ] CHANGELOG created
- [ ] All commits have clear messages
- [ ] No TODOs or debug code left in source

**Total commits**: Should have 10-12 commits across all phases (Phase 0: 2, Phase 1: 3, Phase 2: 6, Phase 3: 3-4)

**Final commit**: `Release version 2.0.0 - Add portal metadata and data fetching`

---

## Post-Implementation (Phases 1-3 Complete)

### What's Next?

**Phase 4**: Browser Automation & Excel Support (PRIORITIZED - October 2025)

Based on user testing, two format-related issues prevent access to significant portions of the portal:
- **Part A**: 182 datasets (6.9%, 147 CSVs) from statistik-berlin-brandenburg.de require JavaScript/browser automation
- **Part B**: 545 datasets (20.6%) have Excel files; 30 datasets (1.14%) are Excel-ONLY

**Implementation approach**: See detailed plan below in Phase 4 section.

**Estimated time**: 8-11 hours (6-8h Puppeteer + 2-3h Excel)
**Complexity**: Part A: Intermediate-Advanced, Part B: Low
**Combined Impact**: +8% portal coverage

---

**Future enhancements** (defer until Phase 4 complete):

**Phase 5**: GeoJSON support
- Parse GeoJSON format
- Extract coordinates and geometries
- Basic spatial operations

**Phase 6**: Advanced filtering
- Query-based data fetching
- Server-side filtering by column values
- Pagination within large datasets

**Phase 7**: Visualization
- Integration with Datawrapper or similar
- Chart generation from fetched data
- Return embed codes or image URLs

**Phase 8**: Analysis tools
- Aggregation functions (group by, sum, average)
- Simple joins across datasets
- Correlation calculations

---

## Phase 4: Browser Automation & Excel Support

This phase addresses two format-related limitations discovered during user testing, unlocking an additional 8% of the portal.

### Part A: Browser Automation for SPA-Hosted Files

**Goal**: Enable fetching of 147 CSV files from statistik-berlin-brandenburg.de that require JavaScript execution.

**Background**: User testing revealed that 6.9% of portal datasets (182 datasets, 147 CSVs) use statistik-berlin-brandenburg.de URLs. This site is a Single Page Application that returns HTML for all URLs and requires JavaScript to download files. Standard HTTP fetching fails.

**Solution**: Optional Puppeteer integration that detects problematic URLs and uses headless Chrome to download files.

**Estimated time**: 6-8 hours

### Task 4.1: Install Puppeteer

**What to do**: Add Puppeteer as an optional dependency.

**Step-by-step**:

1. **Install puppeteer**:
```bash
npm install puppeteer
npm install --save-dev @types/puppeteer
```

2. **Update package.json** to mark as optional in documentation:
```json
{
  "optionalDependencies": {
    "puppeteer": "^21.0.0"
  }
}
```

3. **Verify installation**:
```bash
npm run build
```

**Note**: Puppeteer downloads Chromium (~300MB) on first install.

**Commit message**: `Add puppeteer for browser automation support`

---

### Task 4.2: Create BrowserFetcher Module

**What to do**: Create a new module that uses Puppeteer to download files from JavaScript-rendered pages.

**Files to create**:
- `src/browser-fetcher.ts` - New file

**IMPORTANT IMPLEMENTATION NOTE**: The statistik-berlin-brandenburg.de site uses a SPA that generates dynamic download URLs with hash-based paths (e.g., `download.statistik-berlin-brandenburg.de/{hash1}/{hash2}/file.csv`). Direct response body reading fails due to Puppeteer limitations with cross-origin requests.

**Working solution**: Two-step approach:
1. Navigate to the SPA URL with Puppeteer to trigger JavaScript execution
2. Capture the actual download URL from network traffic using `page.on('response')`
3. Fetch the captured URL directly with node-fetch

**Step-by-step**:

1. **Create the file** `src/browser-fetcher.ts`:

```typescript
// ABOUTME: Downloads files from JavaScript-rendered pages using headless browser
// ABOUTME: Handles Single Page Applications that don't support direct file downloads

import puppeteer, { Browser, Page } from 'puppeteer';

export interface BrowserFetchResult {
  success: boolean;
  data?: string;
  error?: string;
}

export class BrowserFetcher {
  private browser: Browser | null = null;
  private readonly DOWNLOAD_TIMEOUT = 60000; // 60 seconds for browser operations

  async initialize(): Promise<void> {
    if (!this.browser) {
      this.browser = await puppeteer.launch({
        headless: true,
        args: ['--no-sandbox', '--disable-setuid-sandbox'],
      });
    }
  }

  async fetchWithBrowser(url: string): Promise<BrowserFetchResult> {
    let page: Page | null = null;

    try {
      await this.initialize();

      page = await this.browser!.newPage();

      // Strategy: Capture the download URL from network traffic, then fetch it directly
      let downloadUrl: string | null = null;
      const downloadUrlPromise = new Promise<string | null>((resolve) => {
        let resolved = false;

        page!.on('response', async (response) => {
          if (resolved) return;

          const responseUrl = response.url();

          // Look for the actual CSV download URL from the download subdomain
          if (responseUrl.includes('download.statistik-berlin-brandenburg.de') &&
              responseUrl.endsWith('.csv') &&
              response.status() === 200) {
            resolved = true;
            resolve(responseUrl);
          }
        });

        // Timeout after waiting period
        setTimeout(() => {
          if (!resolved) {
            resolved = true;
            resolve(null);
          }
        }, 20000);
      });

      // Navigate to the URL to trigger the SPA
      try {
        await page.goto(url, {
          waitUntil: 'networkidle2',
          timeout: this.DOWNLOAD_TIMEOUT
        });
      } catch (navError) {
        // Navigation might timeout, but we may have captured the download URL
      }

      // Wait for download URL to be captured
      downloadUrl = await downloadUrlPromise;

      await page.close();
      page = null;

      // If we found the download URL, fetch it directly
      if (downloadUrl) {
        try {
          const fetch = (await import('node-fetch')).default;
          const response = await fetch(downloadUrl);

          if (!response.ok) {
            throw new Error(`HTTP ${response.status}: ${response.statusText}`);
          }

          const text = await response.text();

          // Verify it's CSV data
          const trimmed = text.trim();
          if (!trimmed.toLowerCase().startsWith('<!doctype') &&
              !trimmed.toLowerCase().startsWith('<html') &&
              trimmed.length > 0 &&
              (trimmed.includes(',') || trimmed.includes(';'))) {
            return {
              success: true,
              data: text,
            };
          }
        } catch (fetchError) {
          return {
            success: false,
            error: `Found download URL but could not fetch: ${fetchError instanceof Error ? fetchError.message : String(fetchError)}`,
          };
        }
      }

      return {
        success: false,
        error: 'Could not capture download URL from JavaScript-rendered page.',
      };

    } catch (error) {
      if (page) {
        try {
          await page.close();
        } catch (closeError) {
          // Ignore close errors
        }
      }

      return {
        success: false,
        error: error instanceof Error ? error.message : String(error),
      };
    }
  }

  async close(): Promise<void> {
    if (this.browser) {
      await this.browser.close();
      this.browser = null;
    }
  }

  // Check if Puppeteer is available
  static isAvailable(): boolean {
    try {
      require.resolve('puppeteer');
      return true;
    } catch {
      return false;
    }
  }
}
```

**How to test**:

Create a test script `test-browser-fetcher.js`:
```javascript
const { BrowserFetcher } = require('./dist/browser-fetcher.js');

async function test() {
  const fetcher = new BrowserFetcher();

  const result = await fetcher.fetchWithBrowser(
    'https://www.statistik-berlin-brandenburg.de/opendata/EWR_L21_202412E_Matrix.csv'
  );

  console.log('Success:', result.success);
  if (result.error) console.log('Error:', result.error);
  if (result.data) console.log('Got data:', result.data.substring(0, 100));

  await fetcher.close();
}

test();
```

```bash
npm run build
node test-browser-fetcher.js
```

**Expected output**: Should either get CSV data or a clear error.

**Commit message**: `Add BrowserFetcher module for JavaScript-rendered downloads`

---

### Task 4.3: Integrate BrowserFetcher into DataFetcher

**What to do**: Update DataFetcher to detect statistik-berlin-brandenburg.de URLs and use BrowserFetcher when available.

**Files to modify**:
- `src/data-fetcher.ts` - Add browser fallback logic

**Step-by-step**:

1. **Add import** at top of `src/data-fetcher.ts`:
```typescript
import { BrowserFetcher } from './browser-fetcher.js';
```

2. **Add method to detect problematic URLs**:
```typescript
private needsBrowserFetch(url: string): boolean {
  // URLs that require JavaScript execution
  return url.includes('statistik-berlin-brandenburg.de');
}
```

3. **Update fetchResource method** to use browser when needed:

Find the existing `fetchResource` method and update it:

```typescript
async fetchResource(url: string, format: string): Promise<FetchedData> {
  // First try with browser if this URL needs it
  if (this.needsBrowserFetch(url) && BrowserFetcher.isAvailable()) {
    const browserResult = await this.fetchWithBrowser(url, format);
    if (browserResult) return browserResult;
    // If browser fetch failed, fall through to regular fetch
  }

  try {
    // ... existing fetch logic ...
  } catch (error) {
    // ... existing error handling ...
  }
}

private async fetchWithBrowser(url: string, format: string): Promise<FetchedData | null> {
  try {
    const fetcher = new BrowserFetcher();
    const result = await fetcher.fetchWithBrowser(url);
    await fetcher.close();

    if (!result.success || !result.data) {
      console.warn('Browser fetch failed:', result.error);
      return null;
    }

    // Parse the data using existing methods
    return this.parseData(result.data, format, 'text/csv');
  } catch (error) {
    console.error('Browser fetch error:', error);
    return null;
  }
}
```

**How to test**:
```bash
npm run build
node test-fetcher.js  # Should now work with statistik URLs
```

**Commit message**: `Integrate BrowserFetcher for statistik-berlin-brandenburg.de URLs`

---

### Task 4.4: Update Error Messages

**What to do**: Improve error messages to explain browser automation option.

**Files to modify**:
- `src/data-fetcher.ts` - Update HTML detection error

**Step-by-step**:

1. **Update the HTML detection error message**:

```typescript
if (trimmedText.toLowerCase().startsWith('<!doctype html') ||
    trimmedText.toLowerCase().startsWith('<html')) {

  const hasP uppeteer = BrowserFetcher.isAvailable();
  const errorMsg = hasP uppeteer
    ? 'Server returned HTML instead of CSV. Browser automation failed to download the file. The resource may not be accessible programmatically.'
    : 'Server returned HTML instead of CSV. This URL requires browser automation. Install puppeteer (npm install puppeteer) to enable automatic downloads for these files, or download manually from the Berlin Open Data Portal website.';

  return {
    format,
    rows: [],
    totalRows: 0,
    columns: [],
    error: errorMsg,
  };
}
```

**Commit message**: `Improve error messages for browser automation`

---

### Task 4.5: Add Configuration Option

**What to do**: Allow users to disable browser automation if desired.

**Files to modify**:
- `src/data-fetcher.ts` - Add configuration option

**Step-by-step**:

1. **Add constructor parameter**:
```typescript
export class DataFetcher {
  private readonly MAX_DOWNLOAD_SIZE = 50 * 1024 * 1024;
  private readonly REQUEST_TIMEOUT = 30000;
  private readonly useBrowserAutomation: boolean;

  constructor(options: { useBrowserAutomation?: boolean } = {}) {
    this.useBrowserAutomation = options.useBrowserAutomation !== false; // Default true
  }

  // Update needsBrowserFetch check:
  private shouldUseBrowser(url: string): boolean {
    return this.useBrowserAutomation &&
           this.needsBrowserFetch(url) &&
           BrowserFetcher.isAvailable();
  }
}
```

2. **Update index.ts** to pass configuration:
```typescript
// In constructor
this.dataFetcher = new DataFetcher({ useBrowserAutomation: true });
```

**Commit message**: `Add configuration option for browser automation`

---

### Task 4.6: Update Documentation

**What to do**: Document the Puppeteer feature in README and USAGE_EXAMPLES.

**Files to modify**:
- `README.md`
- `docs/USAGE_EXAMPLES.md`

**Step-by-step**:

1. **Update README.md** Installation section:
```markdown
## Installation

```bash
npm install
npm run build
```

### Optional: Browser Automation Support

To enable fetching of datasets from statistik-berlin-brandenburg.de (182 datasets, ~7% of portal), install Puppeteer:

```bash
npm install puppeteer
```

This adds ~300MB of dependencies (Chromium) but unlocks access to demographic and statistical datasets that require JavaScript to download.
```

2. **Update USAGE_EXAMPLES.md** Troubleshooting section:
```markdown
### "Server returned HTML instead of CSV"

Some datasets (especially from statistik-berlin-brandenburg.de) require JavaScript to download.

**Solution**:
1. Install Puppeteer: `npm install puppeteer`
2. Restart the MCP server
3. The server will automatically use browser automation for these URLs

**Alternative**: Download manually from the dataset page
```

**Commit message**: `Document browser automation feature`

---

### Task 4.7: Add Tests

**What to do**: Test browser automation with real statistik URLs.

**Files to modify**:
- `test/integration-test.cjs` - Add browser automation tests

**Step-by-step**:

1. **Add test for statistik URL** (only if Puppeteer installed):

```javascript
// After existing tests, add:
if (BrowserFetcher.isAvailable()) {
  console.log('\nüì± Test 8: Browser automation for statistik URL...');
  try {
    const statistikUrl = 'https://www.statistik-berlin-brandenburg.de/opendata/EWR_L21_202412E_Matrix.csv';
    const fetcher = new DataFetcher({ useBrowserAutomation: true });
    const result = await fetcher.fetchResource(statistikUrl, 'CSV');

    if (result.error) {
      console.log('‚ö†Ô∏è  Browser fetch returned error:', result.error);
    } else {
      console.assert(result.rows.length > 0, 'Should have rows');
      console.log('‚úÖ Browser automation works:', result.rows.length, 'rows');
      passed++;
    }
  } catch (error) {
    console.log('‚ùå Failed:', error.message);
    failed++;
  }
} else {
  console.log('\n‚è≠Ô∏è  Skipping browser automation test (Puppeteer not installed)');
}
```

**How to test**:
```bash
npm run build
node test/integration-test.cjs
```

**Commit message**: `Add integration test for browser automation`

---

### Part B: Excel Format Support (XLS/XLSX)

**Goal**: Enable parsing of Excel files (545 datasets have Excel, 30 are Excel-ONLY).

**Background**: 20.6% of portal datasets include Excel files. While 515 have CSV/JSON alternatives, 30 datasets (1.14% of portal) are only available as Excel. Additionally, supporting Excel improves UX for all 545 datasets.

**Solution**: Add xlsx library to parse Excel files into the same tabular format as CSV.

**Estimated time**: 2-3 hours

---

### Task 4.8: Install xlsx Library

**What to do**: Add xlsx (SheetJS) library for Excel parsing.

**Step-by-step**:

1. **Install xlsx**:
```bash
npm install xlsx
npm install --save-dev @types/xlsx
```

2. **Verify installation**:
```bash
npm run build
```

**Note**: xlsx is only ~2MB, much lighter than Puppeteer.

**Commit message**: `Add xlsx library for Excel file support`

---

### Task 4.9: Add Excel Parsing to DataFetcher

**What to do**: Update DataFetcher to handle Excel files (XLS/XLSX).

**Files to modify**:
- `src/data-fetcher.ts`

**Step-by-step**:

1. **Add import** at top of file:
```typescript
import * as XLSX from 'xlsx';
```

2. **Add parseExcel method** to DataFetcher class:
```typescript
private parseExcel(buffer: Buffer, format: string): FetchedData {
  try {
    // Read the Excel file from buffer
    const workbook = XLSX.read(buffer, { type: 'buffer' });

    // Get first sheet
    const sheetName = workbook.SheetNames[0];
    if (!sheetName) {
      return {
        format,
        rows: [],
        totalRows: 0,
        columns: [],
        error: 'Excel file has no sheets',
      };
    }

    const sheet = workbook.Sheets[sheetName];

    // Convert to JSON (array of objects)
    const rows = XLSX.utils.sheet_to_json(sheet);

    // Extract column names
    const columns = rows.length > 0 ? Object.keys(rows[0]) : [];

    // Sanity check
    if (rows.length === 0 || columns.length === 0) {
      return {
        format,
        rows: [],
        totalRows: 0,
        columns: [],
        error: 'Excel file appears to be empty or has no headers',
      };
    }

    return {
      format: format.toUpperCase(),
      rows,
      totalRows: rows.length,
      columns,
    };
  } catch (error) {
    return {
      format,
      rows: [],
      totalRows: 0,
      columns: [],
      error: `Excel parse error: ${error instanceof Error ? error.message : String(error)}`,
    };
  }
}
```

3. **Update fetchResource method** to handle Excel binary data:

Find the section where response data is fetched and add Excel handling:

```typescript
const contentType = response.headers.get('content-type') || '';

// Handle Excel files - need binary data
const formatLower = format.toLowerCase();
if (formatLower === 'xls' || formatLower === 'xlsx' ||
    contentType.includes('spreadsheet') ||
    contentType.includes('excel') ||
    contentType.includes('ms-excel')) {
  const buffer = await response.arrayBuffer();
  return this.parseExcel(Buffer.from(buffer), format);
}

// For text formats (CSV, JSON), get as text
const text = await response.text();
return this.parseData(text, format, contentType);
```

4. **Add safety check in parseData**:

At the start of `parseData` method:

```typescript
private parseData(text: string, format: string, contentType: string): FetchedData {
  const formatLower = format.toLowerCase();

  // Excel files should not reach here (handled as binary above)
  if (formatLower === 'xls' || formatLower === 'xlsx') {
    return {
      format,
      rows: [],
      totalRows: 0,
      columns: [],
      error: 'Excel files require binary download - internal error',
    };
  }

  // ... rest of existing parseData logic ...
}
```

**How to test**:

Create a quick test:
```bash
npm run build
node -e "
import('./dist/data-fetcher.js').then(async module => {
  const fetcher = new module.DataFetcher();
  // Find an XLSX dataset
  const result = await fetcher.fetchResource('SOME_XLSX_URL', 'XLSX');
  console.log('Rows:', result.totalRows);
  console.log('Columns:', result.columns);
  console.log('Error:', result.error);
});
"
```

**Commit message**: `Add Excel (XLS/XLSX) parsing support to DataFetcher`

---

### Task 4.10: Add Excel Tests and Documentation

**What to do**: Test Excel parsing and document the feature.

**Files to modify**:
- `test/integration-test.cjs`
- `README.md`
- `docs/USAGE_EXAMPLES.md`

**Step-by-step**:

1. **Add Excel test** to integration tests:

In `test/integration-test.cjs`, after existing tests:

```javascript
// Test 8 (or appropriate number): Excel file parsing
console.log('\nüìä Test 8: Parse Excel (XLSX) file...');
try {
  // Search for a dataset with XLSX format
  const xlsxSearch = await api.searchDatasets({ query: 'res_format:XLSX', limit: 1 });

  if (xlsxSearch.results.length > 0 && xlsxSearch.results[0].resources) {
    const dataset = xlsxSearch.results[0];
    const xlsxResource = dataset.resources.find(r => r.format === 'XLSX' || r.format === 'XLS');

    if (xlsxResource) {
      const data = await fetcher.fetchResource(xlsxResource.url, xlsxResource.format);

      if (data.error) {
        console.log('‚ö†Ô∏è  Excel parsing issue:', data.error);
        console.log('   (Some Excel files may have access issues)');
      } else {
        console.assert(data.rows.length > 0, 'Should have rows');
        console.assert(data.columns.length > 0, 'Should have columns');
        console.log('‚úÖ Excel parsing works:', data.rows.length, 'rows,', data.columns.length, 'columns');
        passed++;
      }
    } else {
      console.log('‚è≠Ô∏è  No Excel resource found in dataset');
    }
  } else {
    console.log('‚è≠Ô∏è  No XLSX datasets found');
  }
} catch (error) {
  console.log('‚ùå Failed:', error.message);
  failed++;
}
```

2. **Update README.md** - add to Features section:
```markdown
- üìä **Excel Support**: Automatically parses XLS and XLSX files (545 datasets, 20.6% of portal)
```

3. **Update docs/USAGE_EXAMPLES.md** - add section:
```markdown
## Excel File Support

The server automatically handles Excel files (XLS and XLSX formats):

```
User: "Fetch data from dataset XYZ"
‚Üí Server detects XLSX format
‚Üí Parses first sheet automatically
‚Üí Returns same tabular structure as CSV
```

**Notes**:
- First sheet is used by default
- Headers are detected automatically
- Returns data in same format as CSV (rows + columns)
- Supports both legacy XLS and modern XLSX

**Example**: 545 datasets (20.6% of portal) include Excel files. The server handles them transparently.
```

**How to test**:
```bash
npm run build
node test/integration-test.cjs
```

**Expected output**: Test 8 should pass (or skip gracefully if no Excel files accessible).

**Commit message**: `Add Excel support tests and documentation`

---

### Phase 4 Complete!

**Final checklist**:

**Part A: Browser Automation**
- [x] Puppeteer installed
- [x] BrowserFetcher module created with ABOUTME comments
- [x] DataFetcher integrated with browser fallback
- [x] Browser automation error messages updated
- [x] Configuration option added
- [x] Browser automation tests added

**Part B: Excel Support**
- [x] xlsx library installed
- [x] Excel parsing implemented in DataFetcher
- [x] Binary download handling for Excel files
- [x] Excel tests added
- [x] Excel feature documented

**General**
- [x] All code compiles without errors
- [x] All integration tests pass
- [x] README updated with new features
- [x] USAGE_EXAMPLES updated

**Actual commits**: 7 commits for Phase 4 (1 for libraries, 5 for Excel, 1 for browser automation fix)

**Testing Results** (October 2025):
Successfully tested browser automation with 6 diverse statistik datasets:
- Small files: 447 rows
- Medium files: 542-7,194 rows
- Large files: **388,724 rows** (RBS_OD_ADR.csv)
- Various structures: 8-51 columns
- Different time periods: 2012-2024
- **100% success rate** across all test cases

**Implementation Status**: ‚úÖ **COMPLETE**
- Browser automation working for all tested statistik URLs
- Excel parsing working for XLS/XLSX files
- All 182 statistik datasets (6.9% of portal) now accessible
- All 545 Excel datasets (20.6% of portal) now accessible
- Combined impact: **727 additional datasets** unlocked

**Final commit**: `Fix browser automation for statistik-berlin-brandenburg.de URLs`

---

## Phase 4.5: Browser Download Capability

**Goal**: Enable users to download datasets as files using browser download dialog.

**Background**: After Phase 2 implementation, users could fetch and view data in chat but had no way to save it locally. When users asked "Wo sind die Daten? Ich brauche sie auf meinem Desktop", there was no tool to trigger file downloads. Initially implemented `save_dataset_to_file` with server filesystem paths, but this doesn't work for public web apps - users need browser downloads.

**Solution**: Add `download_dataset` tool that returns file data with special marker, which triggers browser download dialog in interface-prototype.

**Estimated time**: 3-4 hours

---

### Task 4.11: Add download_dataset Tool

**What to do**: Add a new MCP tool that returns file data formatted for browser download.

**Files to modify**:
- `src/index.ts` - Add tool definition and handler

**Step-by-step**:

1. **Add tool definition** in `ListToolsRequestSchema` handler:
```typescript
{
  name: 'download_dataset',
  description: 'DOWNLOAD dataset as a file to the user\'s computer. Triggers browser download dialog. Use when user wants to SAVE/DOWNLOAD the file. Keywords: "herunterladen", "download", "speichern", "save", "auf meinem Computer", "als Datei". Always use this tool when user says they need the data on their computer.',
  inputSchema: {
    type: 'object',
    properties: {
      dataset_id: {
        type: 'string',
        description: 'The dataset ID or name',
      },
      resource_id: {
        type: 'string',
        description: 'Optional: specific resource ID. If not provided, uses first available data resource (CSV/JSON/Excel).',
      },
      format: {
        type: 'string',
        description: 'Output format: "csv" or "json". If not specified, uses resource format.',
        enum: ['csv', 'json'],
      },
    },
    required: ['dataset_id'],
  },
},
```

2. **Add tool handler** in `CallToolRequestSchema` handler:
```typescript
case 'download_dataset': {
  const { dataset_id, resource_id, format: requestedFormat } = args as {
    dataset_id: string;
    resource_id?: string;
    format?: 'csv' | 'json';
  };

  // Get dataset to find resources
  const dataset = await this.api.getDataset(dataset_id);

  if (!dataset.resources || dataset.resources.length === 0) {
    return {
      content: [{
        type: 'text',
        text: `‚ùå No resources available for dataset "${dataset_id}". This dataset may not have downloadable files.`,
      }],
    };
  }

  // Select resource - prefer data formats
  let resource;
  if (resource_id) {
    resource = dataset.resources.find(r => r.id === resource_id);
    if (!resource) {
      return {
        content: [{
          type: 'text',
          text: `‚ùå Resource "${resource_id}" not found. Use \`get_dataset_details\` to see available resources and their IDs.`,
        }],
      };
    }
  } else {
    // Smart resource selection - prefer data formats over HTML/other
    const dataFormats = ['CSV', 'JSON', 'XLSX', 'XLS', 'XML', 'WMS', 'WFS'];
    resource = dataset.resources.find(r =>
      dataFormats.includes(r.format?.toUpperCase())
    ) || dataset.resources[0];
  }

  // Fetch the data
  const fetchedData = await this.dataFetcher.fetchResource(resource.url, resource.format);

  if (fetchedData.error) {
    return {
      content: [{
        type: 'text',
        text: `‚ùå Error downloading data: ${fetchedData.error}\n\nYou can try:\n- Using a different resource\n- Downloading manually from: ${resource.url}`,
      }],
    };
  }

  // Determine output format
  const outputFormat = requestedFormat || (resource.format.toLowerCase() === 'csv' ? 'csv' : 'json');

  // Generate file content
  let fileContent: string;
  let mimeType: string;
  let fileExtension: string;

  if (outputFormat === 'csv') {
    // Convert to CSV
    if (fetchedData.rows.length > 0) {
      const header = fetchedData.columns.join(',') + '\n';
      const rows = fetchedData.rows.map(row => {
        return fetchedData.columns.map(col => {
          const val = row[col];
          // Escape CSV values with commas or quotes
          if (typeof val === 'string' && (val.includes(',') || val.includes('"') || val.includes('\n'))) {
            return `"${val.replace(/"/g, '""')}"`;
          }
          return val ?? '';
        }).join(',');
      }).join('\n');
      fileContent = header + rows;
    } else {
      fileContent = fetchedData.columns.join(',');
    }
    mimeType = 'text/csv';
    fileExtension = 'csv';
  } else {
    // JSON format
    fileContent = JSON.stringify(fetchedData.rows, null, 2);
    mimeType = 'application/json';
    fileExtension = 'json';
  }

  // Generate filename from dataset title
  const safeFilename = dataset.title
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, '-')
    .replace(/^-+|-+$/g, '')
    .substring(0, 50);
  const filename = `${safeFilename}.${fileExtension}`;

  const fileSizeKB = (fileContent.length / 1024).toFixed(2);

  // Return with special marker for download
  let responseText = `‚úÖ **Download ready!**\n\n`;
  responseText += `**Dataset:** ${dataset.title}\n`;
  responseText += `**Format:** ${outputFormat.toUpperCase()}\n`;
  responseText += `**Size:** ${fileSizeKB} KB\n`;
  responseText += `**Rows:** ${fetchedData.rows.length}\n`;
  responseText += `**Columns:** ${fetchedData.columns.length}\n\n`;
  responseText += `[DOWNLOAD:${filename}:${mimeType}]\n`;
  responseText += fileContent;

  return {
    content: [{ type: 'text', text: responseText }],
  };
}
```

**How to test**:
```bash
npm run build
```

Should compile without errors.

**Commit message**: `Add download_dataset tool with browser download support`

---

### Task 4.12: Update Tool Descriptions for Disambiguation

**What to do**: Clarify tool descriptions so Claude doesn't confuse `fetch_dataset_data` and `download_dataset`.

**Files to modify**:
- `src/index.ts` - Update tool descriptions

**Problem**: Both tools fetch data, causing Claude to use the wrong tool. Need clear distinction:
- `fetch_dataset_data` = VIEW in chat for analysis
- `download_dataset` = DOWNLOAD as file to computer

**Step-by-step**:

1. **Update `fetch_dataset_data` description**:
```typescript
description: 'VIEW dataset content in the chat for analysis. Returns a preview (10 sample rows) or full data for small datasets. Use when user wants to SEE/ANALYZE data, not download it. Keywords: "zeig mir", "schau dir an", "wie sieht aus", "analysiere".'
```

2. **Update `download_dataset` description**:
```typescript
description: 'DOWNLOAD dataset as a file to the user\'s computer. Triggers browser download dialog. Use when user wants to SAVE/DOWNLOAD the file. Keywords: "herunterladen", "download", "speichern", "save", "auf meinem Computer", "als Datei". Always use this tool when user says they need the data on their computer.'
```

**Commit message**: `Clarify tool descriptions to prevent fetch vs download confusion`

---

### Task 4.13: Add Browser Download to interface-prototype Backend

**What to do**: Detect `[DOWNLOAD:...]` marker in MCP tool results and send file download message to frontend.

**Files to modify**:
- `backend/src/websocket-handler.ts` - Tool execution callback
- `backend/src/types.ts` - Add FileDownload message type

**Step-by-step**:

1. **Add FileDownload type** to `backend/src/types.ts`:
```typescript
export interface FileDownload {
  type: 'file_download';
  filename: string;
  mimeType: string;
  content: string;
}

// Update WebSocketMessage union
export type WebSocketMessage = ... | FileDownload;
```

2. **Update tool execution callback** in `backend/src/websocket-handler.ts`:
```typescript
async (toolName: string, toolArgs: any) => {
  const result = await this.mcpClient.callTool(toolName, toolArgs);

  // Extract text from MCP result structure
  let resultText = '';
  if (result && result.content && Array.isArray(result.content)) {
    const textContent = result.content.find((item: any) => item.type === 'text');
    if (textContent) {
      resultText = textContent.text;
    }
  }

  // Check for download marker
  const downloadMatch = resultText.match(/\[DOWNLOAD:([^:]+):([^\]]+)\]\n([\s\S]*)/);
  if (downloadMatch) {
    const [, filename, mimeType, fileContent] = downloadMatch;
    const messageBeforeDownload = resultText.substring(0, resultText.indexOf('[DOWNLOAD:'));

    // Send file download to frontend
    this.sendMessage(ws, {
      type: 'file_download',
      filename,
      mimeType,
      content: fileContent
    });

    // Return only message part as tool result
    return {
      content: [{ type: 'text', text: messageBeforeDownload.trim() }]
    };
  }

  return result;
}
```

**Commit message**: `Add file download detection in backend websocket handler`

---

### Task 4.14: Add Browser Download to interface-prototype Frontend

**What to do**: Handle `file_download` messages and trigger browser download using Blob API.

**Files to modify**:
- `frontend/src/lib/Chat.svelte` - WebSocket message handler

**Step-by-step**:

1. **Add download function** in Chat.svelte:
```typescript
function triggerDownload(filename, content, mimeType) {
  // Create Blob from content
  const blob = new Blob([content], { type: mimeType });

  // Create temporary URL
  const url = URL.createObjectURL(blob);

  // Create anchor and trigger download
  const a = document.createElement('a');
  a.href = url;
  a.download = filename;
  document.body.appendChild(a);
  a.click();

  // Clean up
  document.body.removeChild(a);
  URL.revokeObjectURL(url);
}
```

2. **Handle file_download messages** in handleMessage():
```typescript
} else if (data.type === 'file_download') {
  // File download ready - trigger browser download
  triggerDownload(data.filename, data.content, data.mimeType);
}
```

**Commit message**: `Add browser download functionality to frontend`

---

### Task 4.15: Fix Empty Response Bug

**What to do**: Fix bug where responses without tool calls don't appear in frontend.

**Files to modify**:
- `backend/src/claude-client.ts` - sendMessageWithTools method

**Problem**: When Claude responds without tool calls in iteration 1 (non-streaming mode), the response never reaches the frontend because it's not streamed.

**Solution**: Call streamCallback with the response content before breaking the loop.

**Step-by-step**:

```typescript
// If no tool calls, we have final response
if (!response.toolCalls || response.toolCalls.length === 0) {
  finalResponse = response.content;
  // Stream the response if callback provided
  if (streamCallback && response.content) {
    streamCallback(response.content);
  }
  console.log('[ClaudeClient] Final response received, breaking loop');
  break;
}
```

**Commit message**: `Fix empty response bug when no tool calls are made`

---

### Task 4.16: Test Browser Download with Real Datasets

**What to do**: Test the complete flow with interface-prototype.

**Step-by-step**:

1. Start dev server: `npm run dev`
2. Test VIEW in chat: "Zeig mir die Zugriffsstatistik"
   - Should use `fetch_dataset_data`
   - Display sample data in chat
3. Test DOWNLOAD: "Lade die Zugriffsstatistik herunter"
   - Should use `download_dataset`
   - Browser download dialog should appear
   - CSV file should download with auto-generated filename
4. Test JSON download: "Download as JSON"
   - Browser should download .json file
5. Verify no empty responses (the bug we fixed)

---

### Phase 4.5 Complete!

**Final checklist**:
- [x] download_dataset tool implemented with special marker format
- [x] Tool descriptions disambiguated (VIEW vs DOWNLOAD keywords)
- [x] Backend detects [DOWNLOAD:...] marker in tool results
- [x] Frontend triggers browser download with Blob API
- [x] Automatic filename generation from dataset title
- [x] CSV and JSON output formats
- [x] Smart resource selection logic
- [x] Empty response bug fixed
- [x] All changes documented

**Estimated commits**: 5 commits for Phase 4.5:
1. Add download_dataset tool with browser download support
2. Clarify tool descriptions to prevent fetch vs download confusion
3. Add file download detection in backend websocket handler
4. Add browser download functionality to frontend
5. Fix empty response bug when no tool calls are made

**Testing Results** (November 2025):
Successfully tested browser download with various datasets:
- Small files (< 100 rows): Instant download dialog
- Medium files (100-1000 rows): 1-2 seconds to download dialog
- CSV escaping: Handles commas, quotes, newlines correctly
- JSON formatting: Proper indentation and structure
- Filename generation: Clean, readable filenames from dataset titles
- Tool disambiguation: Claude correctly chooses fetch vs download based on user intent
- Empty response bug: Fixed - all responses now appear in chat

**Implementation Status**: ‚úÖ **COMPLETE**
- Browser download working for CSV and JSON formats
- Tool selection working correctly (no more confusion)
- Download marker detection working in backend
- Frontend Blob download working perfectly
- Empty response bug fixed
- Complete workflow: discover ‚Üí view ‚Üí download now possible

**Architecture**:
```
User: "Lade die Daten herunter"
  ‚Üì
Claude identifies download intent ‚Üí calls download_dataset
  ‚Üì
MCP server: Generates file content + marker [DOWNLOAD:filename:mimeType]\ndata
  ‚Üì
Backend: Detects marker, extracts file data, sends file_download message via WebSocket
  ‚Üì
Frontend: Receives file_download, creates Blob, triggers download()
  ‚Üì
Browser: Shows "Save As" dialog, user chooses location and saves
```

**Removed old test script below:**

<details>
<summary>Old filesystem test (no longer needed)</summary>

1. **Old test script** `test-save-file.js` (filesystem-based, replaced by browser download):
```javascript
const { BerlinOpenDataAPI } = require('./dist/berlin-api.js');
const { DataFetcher } = require('./dist/data-fetcher.js');
const path = require('path');
const fs = require('fs');
const os = require('os');

async function testFileSaving() {
  console.log('üß™ Testing file saving functionality\n');

  const api = new BerlinOpenDataAPI();
  const fetcher = new DataFetcher();

  try {
    // Step 1: Find a small dataset
    console.log('Step 1: Searching for test dataset...');
    const results = await api.searchDatasets({ query: 'verkehr', limit: 1 });

    if (results.results.length === 0) {
      console.log('‚ùå No datasets found');
      return;
    }

    const dataset = await api.getDataset(results.results[0].name);
    console.log(`‚úÖ Found dataset: ${dataset.title}`);

    // Step 2: Find a CSV resource
    const csvResource = dataset.resources.find(r => r.format === 'CSV');
    if (!csvResource) {
      console.log('‚ùå No CSV resource found');
      return;
    }
    console.log(`‚úÖ Found CSV resource: ${csvResource.name}`);

    // Step 3: Fetch the data
    console.log('\nStep 2: Fetching data...');
    const data = await fetcher.fetchResource(csvResource.url, csvResource.format);

    if (data.error) {
      console.log(`‚ùå Error fetching: ${data.error}`);
      return;
    }
    console.log(`‚úÖ Fetched ${data.rows.length} rows`);

    // Step 4: Save as CSV
    console.log('\nStep 3: Saving as CSV...');
    const testDir = path.join(os.tmpdir(), 'berlin-mcp-test');
    const csvPath = path.join(testDir, 'test-data.csv');

    // Create directory
    if (!fs.existsSync(testDir)) {
      fs.mkdirSync(testDir, { recursive: true });
    }

    // Write CSV
    const header = data.columns.join(',') + '\n';
    const rows = data.rows.slice(0, 10).map(row => {
      return data.columns.map(col => {
        const val = row[col];
        if (typeof val === 'string' && (val.includes(',') || val.includes('"'))) {
          return `"${val.replace(/"/g, '""')}"`;
        }
        return val ?? '';
      }).join(',');
    }).join('\n');

    fs.writeFileSync(csvPath, header + rows, 'utf8');
    console.log(`‚úÖ Saved to: ${csvPath}`);
    console.log(`   File size: ${(fs.statSync(csvPath).size / 1024).toFixed(2)} KB`);

    // Step 5: Save as JSON
    console.log('\nStep 4: Saving as JSON...');
    const jsonPath = path.join(testDir, 'test-data.json');
    fs.writeFileSync(jsonPath, JSON.stringify(data.rows.slice(0, 10), null, 2), 'utf8');
    console.log(`‚úÖ Saved to: ${jsonPath}`);
    console.log(`   File size: ${(fs.statSync(jsonPath).size / 1024).toFixed(2)} KB`);

    // Step 6: Verify files
    console.log('\nStep 5: Verifying files...');
    const csvExists = fs.existsSync(csvPath);
    const jsonExists = fs.existsSync(jsonPath);
    console.log(`   CSV exists: ${csvExists ? '‚úÖ' : '‚ùå'}`);
    console.log(`   JSON exists: ${jsonExists ? '‚úÖ' : '‚ùå'}`);

    // Cleanup
    console.log('\nCleaning up test files...');
    fs.unlinkSync(csvPath);
    fs.unlinkSync(jsonPath);
    fs.rmdirSync(testDir);
    console.log('‚úÖ Cleanup complete');

    console.log('\n‚úÖ All tests passed!');

  } catch (error) {
    console.error('‚ùå Test failed:', error.message);
    process.exit(1);
  }
}

testFileSaving();
```

2. **Run the test**:
```bash
npm run build
node test-save-file.js
```

**Expected output**: All steps should pass and files should be created and verified.

**Commit message**: `Add tests for save_dataset_to_file functionality`

---

### Task 4.13: Update Documentation

**What to do**: Document the new file saving feature in README and usage examples.

**Files to modify**:
- `README.md`
- `docs/USAGE_EXAMPLES.md`

**Step-by-step**:

1. **Update README.md** - add to Tools section:
```markdown
**Data Fetching & Analysis:**
5. **fetch_dataset_data**: Download and parse dataset contents with smart sampling
6. **save_dataset_to_file**: Save downloaded datasets to local filesystem (CSV/JSON)
```

2. **Update docs/USAGE_EXAMPLES.md** - add new section:
```markdown
### Saving Data to Files

**Question**: "Save the traffic data to my Desktop"

**Workflow**:
1. Search for traffic datasets
2. Get dataset details to see available formats
3. Use `save_dataset_to_file` to download and save

**Tools chain**:
```
search_berlin_datasets("traffic verkehr")
‚Üí get_dataset_details(dataset_id)
‚Üí save_dataset_to_file(dataset_id, "~/Desktop/traffic.csv")
‚Üí Success confirmation with file location
```

**Path options**:
- Home directory: `~/Desktop/data.csv`
- Relative: `./data/traffic.csv`
- Absolute: `/Users/username/Documents/data.csv`

**Supported formats**:
- `.csv` - Comma-separated values with proper escaping
- `.json` - Formatted JSON array

**Features**:
- Automatic directory creation if path doesn't exist
- Path expansion (`~` ‚Üí home directory)
- Smart resource selection (prefers data formats)
- File size and metadata in response
```

3. **Add troubleshooting section**:
```markdown
### "Error saving file"

Common issues:
- **Permission denied**: Check you have write access to the directory
- **Invalid path**: Verify the directory path exists or can be created
- **Disk full**: Check available disk space
- **Unsupported format**: Currently only CSV and JSON are supported
```

**How to test**:
Review the documentation for accuracy and completeness.

**Commit message**: `Document save_dataset_to_file feature in README and usage examples`

---

### Phase 4.5 Complete!

**Final checklist**:
- [x] save_dataset_to_file tool implemented with ABOUTME comments
- [x] Path expansion support (`~` ‚Üí home directory)
- [x] Automatic directory creation
- [x] CSV and JSON output formats
- [x] Smart resource selection logic
- [x] Comprehensive error handling
- [x] File saving tests added
- [x] Documentation updated

**Estimated commits**: 3 commits for Phase 4.5 (1 for tool, 1 for tests, 1 for docs)

**Testing Results** (November 2025):
Successfully tested file saving with various datasets:
- Small files (< 100 rows): Instant save
- Medium files (100-1000 rows): 1-2 seconds
- CSV escaping: Handles commas, quotes, newlines correctly
- JSON formatting: Proper indentation and structure
- Path expansion: `~/Desktop` works correctly
- Directory creation: Nested paths created successfully
- Error handling: Permission and disk space errors caught properly

**Implementation Status**: ‚úÖ **COMPLETE**
- File saving working for CSV and JSON formats
- Path expansion and directory creation working
- Smart resource selection working
- All error cases handled with helpful messages
- Complete workflow: discover ‚Üí fetch ‚Üí analyze ‚Üí save now possible

**Final commit**: `Add file saving capability for downloaded datasets`

---

## Phase 5: Geodata Format Support

**Goal**: Extend format support to handle geospatial data (GeoJSON, KML, WFS), unlocking an additional 25.3% of the portal (674 datasets).

**Background**: Analysis of the Berlin Open Data Portal format distribution reveals that 60.9% of portal datasets (1,620) use geodata formats. Currently only 38.9% of portal datasets are accessible (CSV/JSON/Excel). Adding geodata support represents the largest opportunity for expanding data coverage.

**Format distribution analysis**:
```
High Priority Geodata Formats:
- WFS (596 datasets, 22.4%) - Web Feature Service (queryable vector data)
- KML (39 datasets, 1.5%) - Keyhole Markup Language
- GeoJSON (39 datasets, 1.5%) - JSON-based vector format
- Shapefiles (16+ datasets, 0.6%) - SKIPPED (most have GeoJSON/KML alternatives)
- WMS (933 datasets, 35.1%) - SKIPPED (returns images, not queryable data)

Currently Supported:
- CSV (356 datasets, 13.4%)
- JSON (130 datasets, 4.9%)
- Excel (549 datasets, 20.6%)
Total: 1,035 datasets (38.9%)
```

**Implementation strategy**:
- **Part A**: File-based geodata (GeoJSON, KML) - 78 datasets, 3.0%
- **Part B**: WFS web service support - 596 datasets, 22.4%
- **Shapefiles skipped**: Most datasets provide GeoJSON/KML alternatives
- **WMS skipped**: Returns images not data, limited analysis value

**Target coverage after Phase 5**: 1,709 datasets (64.2%)

**Estimated time**: 8-10 hours total
- GeoJSON: 2-3 hours ‚úÖ COMPLETE
- KML: 2-3 hours ‚úÖ COMPLETE
- WFS: 4-5 hours ‚úÖ COMPLETE

---

### Part A: File-Based Geodata Formats ‚úÖ COMPLETE

**Summary**:
1. ‚úÖ Install geodata libraries (@tmcw/togeojson, @xmldom/xmldom)
2. ‚úÖ Add GeoJSON detection and parsing (features ‚Üí table rows)
3. ‚úÖ Add KML parsing via KML‚ÜíGeoJSON conversion
4. ‚úÖ Test formats with real portal datasets

**Key design**: All geodata converted to tabular format with `geometry_type` and `geometry_coordinates` columns.

### Part B: WFS (Web Feature Service) Support ‚úÖ COMPLETE

**Implementation completed November 2025**

Berlin WFS services follow OGC WFS 2.0.0 standard and return GeoJSON when requested, making them compatible with existing geodata parsing infrastructure.

**URL Landscape**:
Analysis of 1,156 WFS resources revealed 4 host patterns:
- gdi.berlin.de: 1,024 resources (89%) - Standard WFS endpoints
- fbinter.stadt-berlin.de: 127 resources (11%) - FIS Broker services
- energieatlas.berlin.de: 2-3 resources (<1%) - Requires `nodeId` parameter
- Other hosts: 3 resources (<1%)

**Implementation**:

1. **WFSClient module** (`src/wfs-client.ts`):
   - `parseWFSUrl()`: Smart parameter preservation (keeps nodeId, strips WFS params)
   - `getCapabilities()`: Parse XML to discover feature types
   - `getFeatures()`: Fetch features as GeoJSON with pagination
   - `getFeatureCount()`: Get total feature count via RESULTTYPE=hits
   - Case-insensitive parameter filtering for URL variations

2. **Parameter Preservation**:
   - Problem: Initial implementation stripped ALL query params, breaking energieatlas
   - Solution: Preserve service-specific params (nodeId, SRSNAME), only override WFS params
   - WFS params to override: SERVICE, REQUEST, VERSION, TYPENAMES, OUTPUTFORMAT, COUNT, STARTINDEX
   - Preserved params passed through all WFS operations

3. **Coordinate Transformation** (`src/geojson-transformer.ts`):
   - Problem: WFS returns EPSG:25833 (UTM zone 33N), web maps need WGS84 (EPSG:4326)
   - Solution: proj4 library for automatic transformation
   - Detects source CRS from GeoJSON crs property
   - Transforms all geometry types (Point, Polygon, MultiPolygon, etc.)
   - Removes crs property (WGS84 is implicit per GeoJSON spec)
   - Example: [404161.498, 5823125.442] ‚Üí [13.586549, 52.549759]

4. **GeoJSON Structure Preservation**:
   - Problem: Geodata parser flattened to tabular format, losing FeatureCollection structure
   - Solution: `originalGeoJSON` field in FetchedData interface
   - Downloads return proper FeatureCollection with coordinate arrays (not strings)
   - MIME type: application/geo+json for downloads

5. **DataFetcher Integration**:
   - WFS auto-detection via format field or URL pattern
   - Fetches capabilities, selects first feature type
   - Gets total count, fetches first 1000 features
   - Converts to tabular format for display (with originalGeoJSON preserved for downloads)

**Testing**:
- URL parsing: 6/6 test cases pass (clean URLs, with params, with nodeId)
- Coordinate transformation: All 97 Berlin districts validated (lon 13.0-13.8¬∞, lat 52.3-52.7¬∞)
- End-to-end: gdi.berlin.de, fbinter, energieatlas all working
- Download validation: Proper GeoJSON FeatureCollection structure confirmed

**Known Limitations**:
- energieatlas services (2-3) may return GML/XML instead of GeoJSON (future: add GML parser)
- Some services return HTTP 400 for reasons unrelated to implementation

**Coverage impact**: +596 datasets (22.4% of portal), 1,151+ services working (99%+)

---

## Appendix: Debugging Tips

### Common Issues

**"Module not found" errors**:
- Run `npm install` to ensure dependencies are installed
- Check that file extensions include `.js` in imports
- Verify `tsconfig.json` has correct module settings

**"Cannot read property of undefined"**:
- Add null checks before accessing nested properties
- Check API response structure with console.log
- Verify dataset has resources before accessing them

**CSV parsing produces garbage**:
- Papaparse handles most edge cases automatically
- Check if file is actually CSV format
- Verify the resource URL is correct

**Timeout errors**:
- Increase timeout in DataFetcher
- Check if URL is accessible in browser
- File might be too large - verify size first

### Testing Individual Components

**Test API methods**:
```bash
node -e "
const { BerlinOpenDataAPI } = require('./dist/berlin-api.js');
new BerlinOpenDataAPI().getPortalStats().then(console.log);
"
```

**Test data fetcher**:
```bash
node -e "
const { DataFetcher } = require('./dist/data-fetcher.js');
new DataFetcher().fetchResource('URL', 'CSV').then(console.log);
"
```

**Test data sampler**:
```bash
node -e "
const { DataSampler } = require('./dist/data-sampler.js');
const sample = [{ a: 1, b: 'test' }, { a: 2, b: 'test2' }];
console.log(new DataSampler().generateSample(sample, ['a', 'b']));
"
```

---

## Code Style Guidelines

**TypeScript conventions**:
- Use `async/await` over promises
- Prefer `const` over `let`
- Use descriptive variable names
- Add type annotations for public methods
- Keep functions focused and small

**Error handling**:
- Always catch and handle errors
- Provide user-friendly error messages
- Suggest actionable next steps
- Log technical details for debugging

**Comments**:
- Start each file with ABOUTME comments (2 lines)
- Document why, not what
- Explain non-obvious logic
- Don't state the obvious

**Naming**:
- Classes: PascalCase (BerlinOpenDataAPI)
- Methods: camelCase (getDataset)
- Constants: UPPER_SNAKE_CASE (MAX_DOWNLOAD_SIZE)
- Interfaces: PascalCase (PortalStats)

---

## Success Criteria

You'll know you're done when:

‚úÖ All TypeScript compiles without errors
‚úÖ Integration tests pass
‚úÖ You can ask Claude (via Desktop) complex questions and it chains tools correctly
‚úÖ Error messages are helpful and actionable
‚úÖ Documentation is complete and accurate
‚úÖ Code is clean, well-organized, and follows style guidelines
‚úÖ All commits are atomic and well-messaged

**Estimated time**: 8-12 hours for experienced developer

**Difficulty**: Intermediate - requires understanding of:
- REST APIs and HTTP
- Data parsing (CSV/JSON)
- Async JavaScript/TypeScript
- Error handling patterns
- MCP protocol basics

---

## Getting Help

If you get stuck:

1. **Check the docs**: Read existing docs in `docs/` folder
2. **Review existing code**: Look at how similar features are implemented
3. **Test incrementally**: Don't write large chunks without testing
4. **Read error messages carefully**: They usually tell you exactly what's wrong
5. **Search the CKAN API docs**: https://docs.ckan.org/en/latest/api/
6. **Ask for clarification**: If requirements are unclear, ask before implementing

---

## Deployment & Remote Access

### Production Deployment

The Berlin Open Data MCP server is deployed on Railway as part of the unified interface-prototype backend service.

**Deployment Details:**
- Platform: Railway (https://railway.app)
- URL: https://odis-production.up.railway.app
- Deployment: Automatic on push to main branch
- Build command: `npm run build` (builds both MCP server and backend)
- Start command: `npm start` (from root package.json)

**Architecture:**
```
Railway Deployment
‚îú‚îÄ‚îÄ Berlin Open Data MCP Server (built to dist/)
‚îî‚îÄ‚îÄ Interface Prototype Backend
    ‚îú‚îÄ‚îÄ WebSocket handler (for web chat)
    ‚îú‚îÄ‚îÄ Streamable HTTP MCP endpoint (for remote access)
    ‚îî‚îÄ‚îÄ Static frontend (Svelte build)
```

### Remote MCP Access Implementation

The backend exposes the Berlin Open Data MCP server via Streamable HTTP transport at the `/mcp` endpoint.

**Key Implementation Details:**

**Transport:** StreamableHTTPServerTransport (from `@modelcontextprotocol/sdk/server/streamableHttp.js`)
- **Why not SSE:** `mcp-remote` requires Streamable HTTP protocol (2025-03-26), not the deprecated SSE transport (2024-11-05)
- **Endpoint:** Single `/mcp` endpoint handles all operations (GET, POST, DELETE)
- **Session Management:** Via `mcp-session-id` header
- **Initialization:** Client POSTs initialize request without session ID
- **Subsequent Requests:** Client includes session ID in header

**Code Location:** `interface-prototype/backend/src/server.ts`

```typescript
// Store MCP transports by session ID
const mcpTransports: { [sessionId: string]: StreamableHTTPServerTransport } = {};

// Streamable HTTP MCP endpoint (supports GET, POST, DELETE)
app.all('/mcp', async (req, res) => {
  const sessionId = req.headers['mcp-session-id'] as string;
  let transport: StreamableHTTPServerTransport | undefined;

  if (sessionId && mcpTransports[sessionId]) {
    // Reuse existing transport
    transport = mcpTransports[sessionId];
  } else if (!sessionId && req.method === 'POST' && isInitializeRequest(req.body)) {
    // Create new transport for initialization
    const newTransport = new StreamableHTTPServerTransport({
      sessionIdGenerator: () => randomUUID(),
      onsessioninitialized: (sid) => {
        mcpTransports[sid] = newTransport;
      }
    });

    // Connect to Berlin MCP server
    const mcpServer = new BerlinOpenDataMCPServer();
    await mcpServer.connect(newTransport);
    transport = newTransport;
  }

  // Handle the request
  await transport.handleRequest(req, res, req.body);
});
```

**MCP Server Export:** The `BerlinOpenDataMCPServer` class is exported from `berlin-open-data-mcp/src/index.ts` to allow programmatic instantiation with custom transports (not just stdio).

### Usage from Claude Desktop

Users can access the deployed server by adding this configuration:

**macOS:** `~/Library/Application Support/Claude/claude_desktop_config.json`
**Windows:** `%APPDATA%\Claude\claude_desktop_config.json`

```json
{
  "mcpServers": {
    "berlin-data": {
      "command": "npx",
      "args": [
        "mcp-remote",
        "https://odis-production.up.railway.app/mcp"
      ]
    }
  }
}
```

**Requirements:**
- Claude Pro, Team, or Enterprise plan (remote MCP not available on free tier)
- Internet connection
- Restart Claude Desktop after configuration change

### Local Testing

Test the remote MCP endpoint locally before deployment:

```bash
# Start backend
cd interface-prototype
npm run dev:backend

# In another terminal, test with mcp-remote
npx mcp-remote http://localhost:3000/mcp
```

### Troubleshooting Remote Access

**Common Issues:**

1. **"Cannot POST /mcp/sse" error**
   - Cause: Using SSE transport instead of Streamable HTTP
   - Fix: Ensure endpoint is `/mcp` not `/mcp/sse`
   - Fix: Use StreamableHTTPServerTransport not SSEServerTransport

2. **"Server disconnected" error**
   - Check Railway deployment logs
   - Verify endpoint is accessible: `curl https://odis-production.up.railway.app/mcp`
   - Ensure build succeeded and deployed correctly

3. **Session management issues**
   - Verify `mcp-session-id` header is being set
   - Check transport is stored in `mcpTransports` map
   - Ensure cleanup on close is working

**Debug Logs:**

Check Railway logs for:
- `MCP session initialized with ID: <uuid>` (session created)
- `Received POST request to /mcp` (requests received)
- `MCP session <uuid> closed` (cleanup working)

---

## Final Notes

**Remember**:
- DRY: Don't duplicate code
- YAGNI: Build only what's specified
- TDD: Test as you go
- Commit frequently with clear messages
- Keep it simple and readable

**This is a prototype**: Focus on getting it working correctly, not on performance optimization or handling every edge case. We'll iterate based on real usage.

Good luck! üöÄ
